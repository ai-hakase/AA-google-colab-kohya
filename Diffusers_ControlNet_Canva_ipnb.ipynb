{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_ControlNet_Canva_ipnb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDWXJo4TqbdN"
      },
      "source": [
        "## **Diffusers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦ã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚ã€€ï¼ˆï¼—ï¼—æ–‡å­—åˆ¶é™ã‚ã‚Šã€‚ï¼‰**\n",
        "å‚è€ƒ ï¼š\n",
        "[æ—¥æœ¬èª](https://blog.shikoan.com/controlnet_lora/#ControlNet%EF%BC%88%E3%83%9D%E3%83%BC%E3%82%BA%EF%BC%89%E3%82%92%E4%BD%BF%E3%81%86)  ,\n",
        "[è‹±èª](https://huggingface.co/blog/controlnet) ,\n",
        "[Github - AA-google-colab-kohya](https://github.com/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_S2D2.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Driveã«æ¥ç¶š, ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¿½åŠ  { display-mode: \"form\" }\n",
        "\n",
        "# Driveã«æ¥ç¶š\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¿½åŠ \n",
        "# æœ€æ–°ç‰ˆï¼ˆ0.18.0ï¼‰\n",
        "# !pip install git+https://github.com/huggingface/diffusers >/dev/null 2>&1\n",
        "# diffusers==0.17.1\n",
        "!pip install --upgrade diffusers==0.17.1 transformers accelerate scipy ftfy safetensors txt2img >/dev/null 2>&1\n",
        "!pip install k-diffusion >/dev/null 2>&1\n",
        "!pip install --upgrade xformers git+https://github.com/huggingface/accelerate.git >/dev/null 2>&1\n",
        "!pip install opencv-contrib-python >/dev/null 2>&1\n",
        "!pip install controlnet_aux >/dev/null 2>&1\n",
        "\n"
      ],
      "metadata": {
        "id": "2_jumxegTi0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b74b67-dd97-4f1c-8324-49c1ccc6da21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title è‡ªå‹•åˆ‡æ–­ã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ã‚³ãƒ¼ãƒ‰ { display-mode: \"form\" }\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "id": "XesbQoV9Igeq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "28df6441-91df-456c-d563-ae6dc7322eec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ***\n",
        "\n",
        "#@title ControlNet ã®æº–å‚™ { display-mode: \"form\" }\n",
        "\n",
        "# ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¤œç´¢ã™ã‚‹\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# diffusers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "from diffusers import (StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler, StableDiffusionImg2ImgPipeline) # + VAEModel (0.18.0)\n",
        "from diffusers.utils import load_image\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "\n",
        "# controlnet_auxã¨ã„ã†ãƒãƒ¼ã‚ºæ¤œå‡ºã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "# PyTorchã¨ã„ã†æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "import torch\n",
        "\n",
        "# PILã¨ã„ã†ç”»åƒå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "from PIL import Image\n",
        "\n",
        "# JPEGãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
        "# @markdown ***ãƒãƒ¼ã‚¸ãƒ³ã‚°å­¦ç¿’ç”¨ãƒ•ã‚©ãƒ«ãƒ€***\n",
        "dir = \"/content/drive/MyDrive/StableDiffusion/ControlNet/sayapi\" #@param {type: \"string\"}\n",
        "extension = \"*.png\" #@param {type: \"string\"}\n",
        "images_path = os.path.join(dir, extension)\n",
        "# jpeg files ã®ã¿ã‚’æ ¼ç´ã™ã‚‹\n",
        "jpeg_files = glob.glob(images_path)\n",
        "\n",
        "\n",
        "# @markdown ***\n",
        "# @markdown ***ã©ã¡ã‚‰ã‹ã‚’é¸ã¶***\n",
        "\n",
        "# ãƒãƒ¼ã‚ºã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹\n",
        "poses = []\n",
        "\n",
        "isOpenpose = False # @param {type:\"boolean\"}\n",
        "if isOpenpose:\n",
        "    # ãƒãƒ¼ã‚ºã‚’æ¤œå‡ºã™ã‚‹\n",
        "    controlnet_check_path = \"lllyasviel/ControlNet\" #@param {type: \"string\"}\n",
        "    pose_detector = OpenposeDetector.from_pretrained(controlnet_check_path) # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    for file in jpeg_files:\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚ºã‚’æ¤œå‡ºã™ã‚‹\n",
        "        p = pose_detector(Image.open(file))\n",
        "        # ãƒãƒ¼ã‚ºã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "        poses.append(p)\n",
        "    # print(poses)\n",
        "\n",
        "# ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èª­ã¿è¾¼ã‚€ lllyasviel/sd-controlnet-openpose\n",
        "controlnet_model_path = \"lllyasviel/sd-controlnet-openpose\"  #@param {type: \"string\"}\n",
        "\n",
        "isCanny = True # @param {type:\"boolean\"}\n",
        "if isCanny:\n",
        "    # files = os.listdir(dir + \"/\")\n",
        "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹\n",
        "    for file in jpeg_files:\n",
        "        print(file)\n",
        "        # ãƒªã‚¹ãƒˆå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹\n",
        "        image = load_image(file)\n",
        "        image = np.array(image)\n",
        "        # ç”»åƒã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’CV_8Uã«å¤‰æ›ã™ã‚‹\n",
        "        # image = cv2.convertScaleAbs(image)\n",
        "        low_threshold, high_threshold = 100, 200\n",
        "        image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "        image = image[:, :, None]\n",
        "        image = np.concatenate([image, image, image], axis=2)\n",
        "        canny_image = Image.fromarray(image)\n",
        "        canny_image\n",
        "        # ãƒãƒ¼ã‚ºã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "        poses.append(canny_image)\n",
        "    print(poses)\n",
        "    controlnet_model_path = \"lllyasviel/sd-controlnet-canny\"  #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚ãƒ‡ãƒ¼ã‚¿å‹ã¯float16ã«æŒ‡å®šã™ã‚‹\n",
        "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "# @markdown ***\n",
        "\n",
        "# @title Modelã€LoRA ã‚’ StableDiffusionControlNetPipeline ã«è¨­å®š  â€»LoRA ã®å¯¾å¿œãŒã¾ã ã€€{ display-mode: \"form\" }\n",
        "\n",
        "#ãƒ¢ãƒ‡ãƒ«\n",
        "# @markdown ğŸ‘š Model\n",
        "model_path = \"/content/drive/MyDrive/StableDiffusion/Model/kanpiromix_v20\" #@param {type:\"string\"}\n",
        "\n",
        "device=\"cuda\"\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        "    # custom_pipeline=\"lpw_stable_diffusion\",\n",
        ")\n",
        "\n",
        "# VAE ã‚’èª­ã¿è¾¼ã‚€\n",
        "# vae_path = \"stabilityai/sd-vae-ft-mse\"  #@param {type:\"string\"}\n",
        "# vae = VAEModel.from_pretrained(vae_path)\n",
        "\n",
        "# LoRAã‚’è¿½åŠ  /content/drive/MyDrive/StableDiffusion/Lora/japaneseDollLikeness_v15.safetensors\n",
        "# lora_path = \"Kanbara/doll-likeness-series/japaneseDollLikeness_v15.safetensors\" #@param {type:\"string\"}\n",
        "# pipe.unet.load_attn_procs(lora_path)\n",
        "\n",
        "# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’è¨­å®š\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "# @@markdown ***\n",
        "\n",
        "\n",
        "# @@markdown ğŸ‘™ NSFWã‚’è¡¨ç¤ºã™ã‚‹è¨­å®š\n",
        "# Disabling safety checker\n",
        "if pipe.safety_checker is not None:\n",
        "    pipe.safety_checker = lambda images, **kwargs: (images, False)\n",
        "\n",
        "# pipe.safety_checker = lambda images, **kwargs: (images, [True] * len(images))\n",
        "# isSafeNsfwOff = False  #@param {type:\"boolean\"}\n",
        "# if isSafeNsfwOff:\n",
        "#     pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "pipe.to(device)\n",
        "\n",
        "# @markdown ***"
      ],
      "metadata": {
        "id": "A4nXGvyCEW5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99999eee-827a-4648-f828-3d7fcb7b5cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/StableDiffusion/ControlNet/sayapi/3-sayapi.png\n",
            "/content/drive/MyDrive/StableDiffusion/ControlNet/sayapi/2-sayapi.png\n",
            "/content/drive/MyDrive/StableDiffusion/ControlNet/sayapi/1-sayapi.png\n",
            "[<PIL.Image.Image image mode=RGB size=1536x2048 at 0x7949599C2F20>, <PIL.Image.Image image mode=RGB size=1536x2048 at 0x794AD51F4550>, <PIL.Image.Image image mode=RGB size=2048x2048 at 0x794AD536FAC0>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ç”»åƒç”Ÿæˆã‚’å®Ÿè¡Œ { display-mode: \"form\" }\n",
        "\n",
        "# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ã†æ—¥ä»˜ã¨æ™‚åˆ»ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å®šç¾©ã™ã‚‹\n",
        "file_format = \"%Y%m%d_%H%M%S\"\n",
        "i=0\n",
        "\n",
        "# ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—\n",
        "jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "\n",
        "# @markdown ***\n",
        "#txt2imgå‡ºåŠ›ç”»åƒã®ä¿å­˜å…ˆ\n",
        "#@markdown ***ğŸ“‚ å‡ºåŠ›ç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€***\n",
        "save_path = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/sayapi2/\" #@param {type: \"string\"}\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "# @markdown ***\n",
        "\n",
        "#@@markdown ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "prompt = \"(Beautiful woman wearing patterned white knit), Japanese, ((best picture quality)), (super fine), ((ultra fine)), (8K), best picture quality, dark brown long hair, ((color contacts with good color)), lips, one point tattoo, curly hair, blush, small four leaf clover necklace, (Many pierced earrings)\" #@param {type:\"string\"}\n",
        "#@@markdown ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "negative = \"EasyNegative, (worst picture quality:2), sketch, (low quality:2), (normal quality:2),((monochrome)), (bad fingers, bad hands, missing fingers:1.3),nood, nsfw, text, error, missing fingers, extra fingers, few digits, crop, worst picture quality, jpeg image, signature, watermark, Username, blurred, bad feet, (mutation, deformity:1. 3), extra limbs, fused fingers, long neck, bad proportions\" #@param {type:\"string\"}\n",
        "#@markdown ***ğŸ–¼ å‡ºåŠ›æšæ•°***\n",
        "batch_count = 200 #@param {type: \"integer\"}\n",
        "#@markdown ***ğŸ¦µ ã‚¹ãƒ†ãƒƒãƒ—æ•°***\n",
        "steps = 150 #@param {type:\"integer\"}\n",
        "#@@markdown ç”»åƒã‚µã‚¤ã‚º\n",
        "img_width = \"768\" # @param [512, 768, 1024, 1536]\n",
        "img_height = \"1024\" # @param [512, 768, 1024, 1536]\n",
        "#@@markdown CFG\n",
        "CFG = 7 #@param {type: \"number\"}\n",
        "#@@markdown ã‚·ãƒ¼ãƒ‰ï¼ˆ-1ã®æ™‚ã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "if seed is None or seed == -1:\n",
        "  inputSeed = random.randint(0, 2147483647)\n",
        "else:\n",
        "  valueSeed = seed\n",
        "\n",
        "# run stable diffusion\n",
        "images = []\n",
        "image = None\n",
        "generator = torch.Generator(device)\n",
        "poseIndex = 0\n",
        "\n",
        "for i in range(batch_count):\n",
        "\n",
        "  if seed is None or seed == -1:\n",
        "    valueSeed = inputSeed + i\n",
        "  else:\n",
        "    valueSeed = seed\n",
        "  generator.manual_seed(valueSeed)\n",
        "\n",
        "  image = pipe(\n",
        "    prompt,\n",
        "    poses[0],\n",
        "    negative_prompt=negative,\n",
        "    generator=generator,\n",
        "    num_inference_steps=steps,\n",
        "    guidance_scale=CFG,\n",
        "    width=int(img_width),\n",
        "    height=int(img_height),\n",
        "    output_type=\"pil\"\n",
        "    # max_embeddings_multiples=3,\n",
        "  )\n",
        "  poseIndex = random.randint(0, len(poses) - 1)\n",
        "\n",
        "\n",
        "  # ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—\n",
        "  jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  #å‡ºåŠ›ã™ã‚‹ç”»åƒã®åå‰ã‚’ç”Ÿæˆã™ã‚‹\n",
        "  file_name = (jst_now.strftime(file_format)+ \"_\" + str(valueSeed))\n",
        "  image_name = file_name + f\".png\"\n",
        "\n",
        "  #ç”»åƒã‚’ä¿å­˜ã™ã‚‹\n",
        "  save_location = os.path.join(save_path, image_name)\n",
        "\n",
        "  # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
        "  images = image.images\n",
        "\n",
        "  # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®æ•°ã‚’å–å¾—\n",
        "  num_images = len(images)\n",
        "\n",
        "  # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã‚’é †ç•ªã«ä¿å­˜\n",
        "  for i in range(num_images):\n",
        "    # ç”»åƒã‚’å–ã‚Šå‡ºã™\n",
        "    image = images[i]\n",
        "\n",
        "    # @markdown ***\n",
        "    #@markdown ***âœ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿***\n",
        "    save_metadata = True #@param {type: \"boolean\"}\n",
        "    if save_metadata:\n",
        "      metadata = PngInfo()\n",
        "      metadata.add_text(\"prompt\",(prompt))\n",
        "      metadata.add_text(\"negative\",(negative))\n",
        "      metadata.add_text(\"steps\",(str(steps)))\n",
        "      metadata.add_text(\"CFG\",(str(CFG)))\n",
        "      metadata.add_text(\"width\",(str(img_width)))\n",
        "      metadata.add_text(\"height\",(str(img_height)))\n",
        "      metadata.add_text(\"seed\",str((valueSeed)))\n",
        "      image.save(save_location, pnginfo=metadata)\n",
        "\n",
        "    # dst = Image.new('RGB', (image.shape[1], image.shape[0]))\n",
        "    # dst.paste(output, (0, 0))\n",
        "\n",
        "    # # ç”»åƒä¿å­˜ã€‚googleãƒ‰ãƒ©ã‚¤ãƒ–ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã€å¹´æœˆæ—¥æ™‚åˆ†ç§’ã‹ã‚‰ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜\n",
        "    # filename = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y%m%d%H%M%S')\n",
        "    # dst.save(out_dir+'/'+filename+'.png')\n",
        "\n",
        "    # dst # ç”»åƒè¡¨ç¤º\n",
        "\n",
        "    # @markdown ***\n"
      ],
      "metadata": {
        "id": "tJ9NqU4F8UpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OIVx0EDv0Z7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ç”»åƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›\n",
        "\n",
        "import sys\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "#@markdown **ä¿å­˜ã—ãŸç”»åƒã®ãƒ‘ã‚¹**\n",
        "file_dir = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/\"  #@param {type: \"string\"}\n",
        "file_name = \"20230707_235255_101570535.png\" #@param {type: \"string\"}\n",
        "file_path = os.path.join(file_dir, file_name)\n",
        "img = Image.open(file_path)\n",
        "\n",
        "print(\"Prompt: \",img.text[\"prompt\"])\n",
        "print(\"Negative Prompt: \",img.text[\"negative\"])\n",
        "print(\"Scheduler: \", img.text[\"scheduler\"])\n",
        "print(\"Steps: \",img.text[\"steps\"])\n",
        "print(\"CFG: \",img.text[\"CFG\"])\n",
        "print(\"Width: \",img.text[\"width\"])\n",
        "print(\"Height: \",img.text[\"height\"])\n",
        "print(\"Seed: \",img.text[\"seed\"])\n",
        "\n",
        "try:\n",
        "  print(\"Upscaling ratio; \",img.text[\"upscaling ratio\"])\n",
        "  print(\"Up steps: \",img.text[\"up steps\"])\n",
        "  print(\"Denoising strength: \",img.text[\"denoising strength\"])\n",
        "except:\n",
        "  print(\"Hires.fix was OFF.\")\n",
        "\n",
        "# %%writefile output.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}