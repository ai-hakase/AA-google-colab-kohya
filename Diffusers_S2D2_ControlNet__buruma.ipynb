{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_S2D2_ControlNet__buruma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDWXJo4TqbdN"
      },
      "source": [
        "## **Diffusers ライブラリを用いて、画像を生成するスクリプト。**\n",
        "参考 ：\n",
        "[日本語](https://blog.shikoan.com/controlnet_lora/#ControlNet%EF%BC%88%E3%83%9D%E3%83%BC%E3%82%BA%EF%BC%89%E3%82%92%E4%BD%BF%E3%81%86)  ,\n",
        "[英語](https://huggingface.co/blog/controlnet) ,\n",
        "[Github - AA-google-colab-kohya](https://github.com/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_S2D2.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Driveに接続, ライブラリの追加 { display-mode: \"form\" }\n",
        "\n",
        "# Driveに接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ライブラリの追加\n",
        "!pip install --upgrade diffusers==0.17.1 transformers accelerate scipy ftfy safetensors txt2img >/dev/null 2>&1\n",
        "!pip install k-diffusion >/dev/null 2>&1\n",
        "!pip install --upgrade xformers git+https://github.com/huggingface/accelerate.git >/dev/null 2>&1\n",
        "!pip install opencv-contrib-python >/dev/null 2>&1\n",
        "!pip install controlnet_aux >/dev/null 2>&1\n",
        "\n",
        "# 最新版（0.18.0）\n",
        "# !pip install git+https://github.com/huggingface/diffusers"
      ],
      "metadata": {
        "id": "2_jumxegTi0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 自動切断されないようにするコード { display-mode: \"form\" }\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "id": "XesbQoV9Igeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ライブラリをインポート + S2D2 / Embeddingsの準備 { display-mode: \"form\" }\n",
        "%cd /content\n",
        "!git clone https://github.com/keisuke-okb/S2D2 &> /dev/null\n",
        "\n",
        "%cd ./S2D2\n",
        "!git pull\n",
        "!touch __init__.py\n",
        "!pip install -r requirements.txt >/dev/null 2>&1\n",
        "\n",
        "\n",
        "# 各ライブラリをインポート\n",
        "import s2d2\n",
        "\n",
        "# diffusers ライブラリをインポートする\n",
        "import diffusers\n",
        "from diffusers import (StableDiffusionPipeline,StableDiffusionImg2ImgPipeline)\n",
        "\n",
        "# PyTorchという深層学習フレームワークをインポートする\n",
        "import torch\n",
        "\n",
        "\n",
        "class StableDiffusionImageGeneratorMod(s2d2.StableDiffusionImageGenerator):\n",
        "  def __init__(self, sd_safetensor_path: str, device: str=\"cuda\", dtype: torch.dtype=torch.float16, controlnet=None, vae=None):\n",
        "    self.device = torch.device(device)\n",
        "    self.controlnet = controlnet # ここで属性を作る\n",
        "    self.vae = vae\n",
        "    self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      sd_safetensor_path,\n",
        "      torch_dtype=dtype,\n",
        "      custom_pipeline=\"lpw_stable_diffusion\"\n",
        "    ).to(device)\n",
        "    self.pipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "      sd_safetensor_path,\n",
        "      torch_dtype=dtype,\n",
        "      custom_pipeline=\"lpw_stable_diffusion\"\n",
        "    ).to(device)\n",
        "    self.pipe.safety_checker = None\n",
        "    self.pipe_i2i.safety_checker = None\n",
        "    return\n",
        "\n",
        "  def load_embeddings(self, safetensors_path: str, fileName: str, token: str):\n",
        "    if token != \"\":\n",
        "      self.pipe.load_textual_inversion(safetensors_path, weight_name=fileName, token=token)\n",
        "      self.pipe_i2i.load_textual_inversion(safetensors_path, weight_name=fileName, token=token)\n",
        "    else:\n",
        "      self.pipe.load_textual_inversion(safetensors_path, weight_name=fileName)\n",
        "      self.pipe_i2i.load_textual_inversion(safetensors_path, weight_name=fileName)\n",
        "    return\n",
        "\n",
        "  %cd /content/\n"
      ],
      "metadata": {
        "id": "BFwHNs_9xLCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ControlNet の準備 { display-mode: \"form\" }\n",
        "\n",
        "# ワイルドカードを使ってファイルパスを検索する\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# diffusers ライブラリをインポートする\n",
        "from diffusers import (StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler, StableDiffusionImg2ImgPipeline) # + VAEModel (0.18.0)\n",
        "from diffusers.utils import numpy_to_pil\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "\n",
        "# controlnet_auxというポーズ検出のライブラリをインポートする\n",
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "# PyTorchという深層学習フレームワークをインポートする\n",
        "import torch\n",
        "\n",
        "# PILという画像処理ライブラリをインポートする\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# JPEGファイルのパスのリストを取得\n",
        "# @markdown ***ポージング学習用フォルダ***\n",
        "dir = \"/content/drive/MyDrive/StableDiffusion/ControlNet/spread_legs\" #@param {type: \"string\"}\n",
        "wildcard = \"/*.\"\n",
        "extension = \"png\" #@param {type: \"string\"}\n",
        "images_path = os.path.join(dir, wildcard, extension)\n",
        "jpeg_files = glob.glob(images_path)\n",
        "\n",
        "# ポーズを検出する\n",
        "controlnet_check_path = \"lllyasviel/ControlNet\" #@param {type: \"string\"}\n",
        "pose_detector = OpenposeDetector.from_pretrained(controlnet_check_path) # 事前学習済みのモデルを読み込む\n",
        "# ポーズのリストを作成する\n",
        "poses = []\n",
        "for file in jpeg_files:\n",
        "    # ファイルパスから画像を読み込んでポーズを検出する\n",
        "    p = pose_detector(Image.open(file))\n",
        "    # ポーズをリストに追加する\n",
        "    poses.append(p)\n",
        "\n",
        "# コントロールネットワークを読み込む\n",
        "controlnet_model_path = \"lllyasviel/sd-controlnet-openpose\"  #@param {type: \"string\"}\n",
        "# 事前学習済みのモデルを読み込む。データ型はfloat16に指定する\n",
        "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
        "\n"
      ],
      "metadata": {
        "id": "6CJX5oAxWWEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model、LoRA を StableDiffusionImageGeneratorMod に設定 { display-mode: \"form\" }\n",
        "\n",
        "#モデル\n",
        "# @markdown **👚 Model ########################################################################**\n",
        "model_path = \"/content/drive/MyDrive/StableDiffusion/Model/chilled_remix_v2\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# VAE を読み込む\n",
        "# vae_path = \"stabilityai/sd-vae-ft-mse\"  #@param {type:\"string\"}\n",
        "# vae = VAEModel.from_pretrained(vae_path)\n",
        "\n",
        "generator = StableDiffusionImageGeneratorMod(\n",
        "  model_path,\n",
        "  # vae=vae,\n",
        "  controlnet=controlnet,\n",
        "  device=\"cuda\"\n",
        ")\n",
        "\n",
        "# @markdown **<br>🙎 LoRA ########################################################################**\n",
        "LoRA_USE_1 = True #@param {type:\"boolean\"}\n",
        "if LoRA_USE_1 == True:\n",
        "  LoRA_1=\"/content/drive/MyDrive/StableDiffusion/Lora/flat2.safetensors\" #@param {type:\"string\"}\n",
        "  LoRA_alpha_1 = -1.0 #@param {type:\"number\"}\n",
        "  generator.load_lora(LoRA_1, alpha=LoRA_alpha_1)\n",
        "\n",
        "LoRA_USE_2= True #@param {type:\"boolean\"}\n",
        "if LoRA_USE_2== True:\n",
        "  LoRA_2=\"/content/drive/MyDrive/StableDiffusion/Lora/koreanDollLikeness_v20.safetensors\" #@param {type:\"string\"}\n",
        "  LoRA_alpha_2 = 0.6 #@param {type:\"number\"}\n",
        "  generator.load_lora(LoRA_2, alpha=LoRA_alpha_2)\n",
        "\n",
        "LoRA_USE_3= True #@param {type:\"boolean\"}\n",
        "if LoRA_USE_3== True:\n",
        "  LoRA_3=\"/content/drive/MyDrive/StableDiffusion/Lora/trueBuruma_v26red.safetensors\" #@param {type:\"string\"}\n",
        "  LoRA_alpha_3 = 0.1 #@param {type:\"number\"}\n",
        "  generator.load_lora(LoRA_3, alpha=LoRA_alpha_3)\n",
        "\n",
        "LoRA_USE_4= False #@param {type:\"boolean\"}\n",
        "if LoRA_USE_4== True:\n",
        "  LoRA_4=\"/content/drive/MyDrive/Lora/EkunePOVFellatioV2.safetensors\" #@param {type:\"string\"}\n",
        "  LoRA_alpha_4 = 0.5 #@param {type:\"number\"}\n",
        "  generator.load_lora(LoRA_4, alpha=LoRA_alpha_4)\n",
        "\n",
        "\n",
        "#@markdown **<br>👾 Embeddings ########################################################################**\n",
        "embeddings_USE_1 = True #@param {type:\"boolean\"}\n",
        "if embeddings_USE_1 == True:\n",
        "  safetensors_path_1 = \"/content/drive/MyDrive/StableDiffusion/embeddings/EasyNegative.safetensors\" #@param {type:\"string\"}\n",
        "  file_name_1 = \"EasyNegative.safetensors\" #@param {type:\"string\"}\n",
        "  token_1 = \"EasyNegative\" #@param {type:\"string\"}\n",
        "  generator.load_embeddings(safetensors_path_1, file_name_1, token_1)\n",
        "\n",
        "embeddings_USE_2 = True #@param {type:\"boolean\"}\n",
        "if embeddings_USE_2 == True:\n",
        "  safetensors_path_2 = \"/content/drive/MyDrive/StableDiffusion/embeddings/negative_hand-neg.pt\" #@param {type:\"string\"}\n",
        "  file_name_2 = \"negative_hand-neg.pt\" #@param {type:\"string\"}\n",
        "  token_2 = \"negative_hand-neg\" #@param {type:\"string\"}\n",
        "  generator.load_embeddings(safetensors_path_2, file_name_2, token_2)\n",
        "\n",
        "embeddings_USE = True #@param {type:\"boolean\"}\n",
        "if embeddings_USE == True:\n",
        "  safetensors_path_3 = \"/content/drive/MyDrive/StableDiffusion/embeddings/pureerosface_v1.pt\" #@param {type:\"string\"}\n",
        "  file_name_3 = \"pureerosface_v1.pt\" #@param {type:\"string\"}\n",
        "  token_3 = \"pureerosface_v1\" #@param {type:\"string\"}\n",
        "  generator.load_embeddings(safetensors_path_3, file_name_3, token_3)\n",
        "\n",
        "embeddings_USE = True #@param {type:\"boolean\"}\n",
        "if embeddings_USE == True:\n",
        "  safetensors_path_4 = \"/content/drive/MyDrive/StableDiffusion/embeddings/ng_deepnegative_v1_75t.pt\" #@param {type:\"string\"}\n",
        "  file_name_4 = \"ng_deepnegative_v1_75t.pt\" #@param {type:\"string\"}\n",
        "  token_4 = \"ng_deepnegative_v1_75t\" #@param {type:\"string\"}\n",
        "  generator.load_embeddings(safetensors_path_4, file_name_4, token_4)\n",
        "\n",
        "# embeddings_USE = True #@param {type:\"boolean\"}\n",
        "# if embeddings_USE == True:\n",
        "#   safetensors_path_5 = \"/content/drive/MyDrive/StableDiffusion/embeddings/negative_hand-neg.pt\" #@param {type:\"string\"}\n",
        "#   file_name_5 = \"negative_hand-neg.pt\" #@param {type:\"string\"}\n",
        "#   token_5 = \"negative_hand-neg\" #@param {type:\"string\"}\n",
        "#   generator.load_embeddings(safetensors_path_5, file_name_5, token_5)\n",
        "\n"
      ],
      "metadata": {
        "id": "tJ9NqU4F8UpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYM1Dv3QeR08"
      },
      "outputs": [],
      "source": [
        "# @title 画像を生成  { display-mode: \"form\" }\n",
        "\n",
        "# 必要なモジュールやライブラリをインポートする\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "# ファイル名に使う日付と時刻のフォーマットを定義する\n",
        "file_format = \"%Y%m%d_%H%M%S\"\n",
        "i=0\n",
        "\n",
        "# 現在の日本時間を取得\n",
        "jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "#txt2img出力画像の保存先\n",
        "#@markdown ***出力画像を保存するフォルダ***\n",
        "save_path = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/buruma/\" #@param {type: \"string\"}\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "#@@markdown ポジティブプロンプト\n",
        "prompt = \"Beautiful girl,(See-through clothes very wet with water,sitting,very wet body:1.3),(red bloomers,red gym clothes, under size clothes),(camel toe, exposure pants:1.3 ),short brown hair,((super detail)), photo realistic, 8K UHD,(best quality:1. 2), high resolution, fine details, fine textures, facial expression, high details, delicate and beautiful eyes, perfectly proportioned face, light particles, clear_image, high resolution, high quality textures and shadows, realistic and beautiful face with big eyes, blush, glossy lips and perfect proportions, depth of field, lens flare, ray tracing, perspective,glossy skin, (smiling, shy),low angle,shiny hands, nfsw,see through nipples, see through pants\" #@param {type:\"string\"}\n",
        "#@@markdown ネガティブプロンプト\n",
        "negative = \"EasyNegative, painting, sketch, (worst picture quality:2), (low quality:2), (normal quality:2), bad feet, lowres, ((monochrome)),(ng_deepnegative_v1_75t),(negative_hand-neg:1.2),skin blemishes, acne, skin blemishes, bad anatomy, (bad fingers, bad hands, missing fingers:1.3), text, error, missing fingers, extra fingers, few digits, crop, worst picture quality, jpeg image, signature, watermark, Username, blurred, bad feet, (mutation, deformity:1. 3), extra limbs, fused fingers, long neck, crossed eyes, very low resolution, bad proportions\" #@param {type:\"string\"}\n",
        "#@markdown ***出力枚数***\n",
        "batch_count = 200 #@param {type: \"integer\"}\n",
        "#@markdown ***ステップ数***\n",
        "steps = 30 #@param {type:\"integer\"}\n",
        "#@@markdown 画像サイズ\n",
        "img_width = \"512\" # @param [512, 768, 1024, 1536]\n",
        "img_height = \"768\" # @param [512, 768, 1024, 1536]\n",
        "#@@markdown CFG\n",
        "CFG = 7 #@param {type: \"number\"}\n",
        "#@markdown ***スケジューラ(サンプラー)***\n",
        "scheduler=\"dpm++_2m_karras\" #@param [\"unipc\",\"euler_a\",\"euler\",\"ddim\",\"ddpm\",\"deis\",\"dpm2\",\"dpm++_2s\",\"dpm++_2m\",\"dpm++_2m_karras\",\"dpm++_sde\",\"dpm++_sde_karras\",\"heun\",\"heun_karras\",\"lms\",\"lms_karras\",\"pndm\",\"dpm++_2m_karras\"]\n",
        "#@@markdown シード（-1の時はランダム）\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "if seed is None or seed == -1:\n",
        "  inputSeed = random.randint(0, 2147483647)\n",
        "else:\n",
        "  valueSeed = seed\n",
        "\n",
        "#@markdown ***Hires.fix の有効化***\n",
        "hires_fix = True #@param {type: \"boolean\"}\n",
        "enhance_steps = 2 if hires_fix else 1\n",
        "#@markdown ***解像度倍率(乗算後、最も近い8の倍数のサイズとなる)***\n",
        "upscaling_ratio = 2 #@param {type: \"number\"}\n",
        "#@markdown ***アップスケールステップ数***\n",
        "up_steps = 20 #@param {type: \"integer\"}\n",
        "#@markdown ***小さいほど元画像を尊重） （0.5～0.7）が推奨***\n",
        "denoising_strength = 0.60 #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "for i in range(batch_count):\n",
        "  if seed is None or seed == -1:valueSeed = inputSeed + i\n",
        "  image = generator.diffusion_enhance(\n",
        "    prompt,\n",
        "    negative,\n",
        "    scheduler_name=scheduler,\n",
        "    num_inference_steps=steps,\n",
        "    num_inference_steps_enhance=up_steps,\n",
        "    guidance_scale=CFG,\n",
        "    width=img_width,\n",
        "    height=img_height,\n",
        "    seed=valueSeed,\n",
        "    upscale_target=\"latent\",\n",
        "    interpolate_mode=\"bicubic\",\n",
        "    antialias=True,\n",
        "    # diffusion_enhance=True,\n",
        "    upscale_by=upscaling_ratio,\n",
        "    enhance_steps=enhance_steps,\n",
        "    denoising_strength=denoising_strength,\n",
        "    output_type=\"pil\",\n",
        "    decode_factor=0.15,\n",
        "    decode_factor_final=0.18215\n",
        "  )\n",
        "\n",
        "  # 現在の日本時間を取得\n",
        "  jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  #出力する画像の名前を生成する\n",
        "  file_name = (jst_now.strftime(file_format)+ \"_\" + str(valueSeed))\n",
        "  image_name = file_name + f\".png\"\n",
        "\n",
        "  #画像を保存する\n",
        "  save_location = os.path.join(save_path, image_name)\n",
        "\n",
        "  #@markdown ***メタデータの書き込み***\n",
        "  save_metadata = True #@param {type: \"boolean\"}\n",
        "  if save_metadata:\n",
        "    metadata = PngInfo()\n",
        "    metadata.add_text(\"prompt\",(prompt))\n",
        "    metadata.add_text(\"negative\",(negative))\n",
        "    metadata.add_text(\"scheduler\",(scheduler))\n",
        "    metadata.add_text(\"steps\",(str(steps)))\n",
        "    metadata.add_text(\"CFG\",(str(CFG)))\n",
        "    metadata.add_text(\"width\",(str(img_width)))\n",
        "    metadata.add_text(\"height\",(str(img_height)))\n",
        "    metadata.add_text(\"seed\",str((valueSeed)))\n",
        "  if hires_fix:\n",
        "    metadata.add_text(\"upscaling ratio\",str((upscaling_ratio)))\n",
        "    metadata.add_text(\"up steps\",str((up_steps)))\n",
        "    metadata.add_text(\"denoising strength\",str((denoising_strength)))\n",
        "    image.save(save_location, pnginfo=metadata)\n",
        "  else:\n",
        "    image.save(save_location)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OIVx0EDv0Z7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 画像のメタデータを出力\n",
        "\n",
        "import sys\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "#@markdown **保存した画像のパス**\n",
        "file_dir = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/test2/\"  #@param {type: \"string\"}\n",
        "file_name = \"20230710_170302_1067497271.png\" #@param {type: \"string\"}\n",
        "file_path = os.path.join(save_path, file_name)\n",
        "img = Image.open(file_path)\n",
        "\n",
        "print(\"Prompt: \",img.text[\"prompt\"])\n",
        "print(\"Negative Prompt: \",img.text[\"negative\"])\n",
        "print(\"Scheduler: \", img.text[\"scheduler\"])\n",
        "print(\"Steps: \",img.text[\"steps\"])\n",
        "print(\"CFG: \",img.text[\"CFG\"])\n",
        "print(\"Width: \",img.text[\"width\"])\n",
        "print(\"Height: \",img.text[\"height\"])\n",
        "print(\"Seed: \",img.text[\"seed\"])\n",
        "\n",
        "try:\n",
        "  print(\"Upscaling ratio; \",img.text[\"upscaling ratio\"])\n",
        "  print(\"Up steps: \",img.text[\"up steps\"])\n",
        "  print(\"Denoising strength: \",img.text[\"denoising strength\"])\n",
        "except:\n",
        "  print(\"Hires.fix was OFF.\")\n",
        "\n",
        "# %%writefile output.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}