{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_t2i_i2i_controlnet_chose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDWXJo4TqbdN"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Diffusers ライブラリを用いて、画像を生成するスクリプト。**\n",
        "参考 ：\n",
        "[日本語](https://blog.shikoan.com/controlnet_lora/#ControlNet%EF%BC%88%E3%83%9D%E3%83%BC%E3%82%BA%EF%BC%89%E3%82%92%E4%BD%BF%E3%81%86)  ,\n",
        "[英語](https://huggingface.co/blog/controlnet) ,\n",
        "[Github - AA-google-colab-kohya](https://github.com/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_S2D2.ipynb)\n",
        "\n",
        "[※現時点で、Controlnet とは併用できない。](https://github.com/keisuke-okb/S2D2/tree/main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2_jumxegTi0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9c52e677-edd8-4c64-c0dc-e2e0098476f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Driveに接続, ライブラリの追加 { display-mode: \"form\" }\n",
        "\n",
        "# Driveに接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ライブラリの追加\n",
        "# 最新版（0.18.0）\n",
        "!pip install git+https://github.com/huggingface/diffusers.git@v0.18.0 >/dev/null 2>&1\n",
        "!pip install --upgrade transformers accelerate scipy ftfy xformers safetensors txt2img >/dev/null 2>&1 # diffusers==0.17.1\n",
        "!pip install k-diffusion OmegaConf opencv-contrib-python controlnet_aux >/dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XesbQoV9Igeq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "collapsed": true,
        "outputId": "ea2d2c17-4382-4792-aa68-536f19617db7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title 自動切断されないようにするコード { display-mode: \"form\" }\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFwHNs_9xLCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d08b353f-c684-46b9-f047-687e27a8a160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/S2D2\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# @markdown ***\n",
        "#@title ライブラリをインポート + S2D2 / Embeddingsの準備 { display-mode: \"form\" }\n",
        "%cd /content\n",
        "!git clone https://github.com/keisuke-okb/S2D2 &> /dev/null\n",
        "\n",
        "%cd ./S2D2\n",
        "!git pull\n",
        "!touch __init__.py\n",
        "!pip install -r requirements.txt >/dev/null 2>&1\n",
        "\n",
        "\n",
        "# 各ライブラリをインポート\n",
        "import s2d2\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# diffusers ライブラリをインポートする\n",
        "import diffusers\n",
        "from diffusers import (StableDiffusionPipeline, StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler, StableDiffusionImg2ImgPipeline)\n",
        "from diffusers.utils import load_image, numpy_to_pil\n",
        "# from diffusers.utils import register_pipeline\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "from diffusers.models import AutoencoderKL\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# controlnet_auxというポーズ検出のライブラリをインポートする\n",
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "# PyTorchという深層学習フレームワークをインポートする\n",
        "import torch\n",
        "\n",
        "# PILという画像処理ライブラリをインポートする\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "##################################################\n",
        "\n",
        "\n",
        "device=\"cuda\"\n",
        "\n",
        "\n",
        "SCHEDULERS = {\n",
        "    \"unipc\": diffusers.schedulers.UniPCMultistepScheduler,\n",
        "    \"euler_a\": diffusers.schedulers.EulerAncestralDiscreteScheduler,\n",
        "    \"euler\": diffusers.schedulers.EulerDiscreteScheduler,\n",
        "    \"ddim\": diffusers.schedulers.DDIMScheduler,\n",
        "    \"ddpm\": diffusers.schedulers.DDPMScheduler,\n",
        "    \"deis\": diffusers.schedulers.DEISMultistepScheduler,\n",
        "    \"dpm2\": diffusers.schedulers.KDPM2DiscreteScheduler,\n",
        "    \"dpm2-a\": diffusers.schedulers.KDPM2AncestralDiscreteScheduler,\n",
        "    \"dpm++_2s\": diffusers.schedulers.DPMSolverSinglestepScheduler,\n",
        "    \"dpm++_2m\": diffusers.schedulers.DPMSolverMultistepScheduler,\n",
        "    \"dpm++_2m_karras\": diffusers.schedulers.DPMSolverMultistepScheduler,\n",
        "    \"dpm++_sde\": diffusers.schedulers.DPMSolverSDEScheduler,\n",
        "    \"dpm++_sde_karras\": diffusers.schedulers.DPMSolverSDEScheduler,\n",
        "    \"heun\": diffusers.schedulers.HeunDiscreteScheduler,\n",
        "    \"heun_karras\": diffusers.schedulers.HeunDiscreteScheduler,\n",
        "    \"lms\": diffusers.schedulers.LMSDiscreteScheduler,\n",
        "    \"lms_karras\": diffusers.schedulers.LMSDiscreteScheduler,\n",
        "    \"pndm\": diffusers.schedulers.PNDMScheduler,\n",
        "}\n",
        "\n",
        "def calc_pix_8(x):\n",
        "    x = int(x)\n",
        "    return x - x % 8\n",
        "\n",
        "\n",
        "##################################################\n",
        "\n",
        "# @markdown ***t2i, i2i, controlnet のどれを使用するか選ぶ***\n",
        "class isMode:\n",
        "    on_t2i = False #@param {type: \"boolean\"}\n",
        "    on_i2i = False #@param {type: \"boolean\"}\n",
        "    on_controlnet = True #@param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "class StableDiffusionImageGeneratorMod(s2d2.StableDiffusionImageGenerator):\n",
        "  def __init__(self, sd_safetensor_path: str, controlnet=None, vae=None, dtype: torch.dtype=torch.float16):\n",
        "\n",
        "    self.device = torch.device(device)\n",
        "    self.controlnet = controlnet\n",
        "    self.vae = vae\n",
        "\n",
        "    match True: # どれか一つの case にマッチさせるために True を使います\n",
        "\n",
        "        case isMode.on_t2i:\n",
        "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                sd_safetensor_path,\n",
        "                torch_dtype=dtype,\n",
        "                vae=vae,\n",
        "                custom_pipeline=\"lpw_stable_diffusion\",\n",
        "            ).to(device)\n",
        "\n",
        "        case isMode.on_i2i:\n",
        "            self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "                sd_safetensor_path,\n",
        "                torch_dtype=dtype,\n",
        "                vae=vae,\n",
        "                custom_pipeline=\"lpw_stable_diffusion\",\n",
        "            ).to(device)\n",
        "\n",
        "        case isMode.on_controlnet:\n",
        "            self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "                sd_safetensor_path,\n",
        "                controlnet=controlnet,\n",
        "                torch_dtype=dtype,\n",
        "                vae=vae,\n",
        "                # custom_pipeline=\"lpw_stable_diffusion\", # → ない。\n",
        "            ).to(device)\n",
        "\n",
        "        # case _: # デフォルト処理として on_t2i を選択します\n",
        "        #     self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        #         sd_safetensor_path,\n",
        "        #         torch_dtype=dtype,\n",
        "        #         vae=vae,\n",
        "        #         custom_pipeline=\"lpw_stable_diffusion\",\n",
        "        #     ).to(device)\n",
        "\n",
        "    # Hires.fix\n",
        "    self.pipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "      sd_safetensor_path,\n",
        "      vae=vae,\n",
        "      torch_dtype=dtype,\n",
        "      custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    ).to(device)\n",
        "\n",
        "    # NFSWを無効にする\n",
        "    self.pipe.safety_checker = None\n",
        "    self.pipe_i2i.safety_checker = None\n",
        "    # 画像生成を早くする\n",
        "    self.pipe.enable_xformers_memory_efficient_attention()\n",
        "    self.pipe_i2i.enable_xformers_memory_efficient_attention()\n",
        "    return\n",
        "\n",
        "  def load_embeddings(self, safetensors_path: str, fileName: str, token: str):\n",
        "    if token != \"\":\n",
        "      self.pipe.load_textual_inversion(safetensors_path, weight_name=fileName, token=token)\n",
        "      self.pipe_i2i.load_textual_inversion(safetensors_path, weight_name=fileName, token=token)\n",
        "    else:\n",
        "      self.pipe.load_textual_inversion(safetensors_path, weight_name=fileName)\n",
        "      self.pipe_i2i.load_textual_inversion(safetensors_path, weight_name=fileName)\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def decode_latents_to_PIL_image(self, latents, decode_factor=0.18215):\n",
        "      with torch.no_grad():\n",
        "          latents = 1 / decode_factor * latents\n",
        "          image = self.pipe.vae.decode(latents).sample\n",
        "          image = (image / 2 + 0.5).clamp(0, 1)\n",
        "          image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "          image = numpy_to_pil(image)\n",
        "          image = StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None).images[0]\n",
        "          return image\n",
        "\n",
        "\n",
        "  def diffusion_from_noise(\n",
        "          self,\n",
        "          prompt,\n",
        "          source_image,\n",
        "          negative_prompt,\n",
        "          scheduler_name=\"dpm++_2m_karras\",\n",
        "          num_inference_steps=20,\n",
        "          guidance_scale=9.5,\n",
        "          width=512,\n",
        "          height=512,\n",
        "          output_type=\"pil\",\n",
        "          decode_factor=0.18215,\n",
        "          seed=1234,\n",
        "          max_embeddings_multiples=None,\n",
        "          save_path=None\n",
        "          ):\n",
        "\n",
        "      # # スケジューラーを設定\n",
        "      # pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "      self.pipe.scheduler = SCHEDULERS[scheduler_name].from_config(self.pipe.scheduler.config)\n",
        "      self.pipe.scheduler.set_timesteps(num_inference_steps, self.device)\n",
        "      seed = random.randint(1, 1000000000) if seed == -1 else seed\n",
        "\n",
        "      with torch.no_grad():\n",
        "          match True: # どれか一つの case にマッチさせるために True を使います\n",
        "\n",
        "              case isMode.on_t2i:\n",
        "                  latents = self.pipe(\n",
        "                      prompt=prompt,\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      generator=torch.manual_seed(seed),\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      output_type=\"latent\"\n",
        "                  ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "              case isMode.on_i2i:\n",
        "                  latents = self.pipe(\n",
        "                      prompt=prompt,\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      image=source_image,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      generator=torch.manual_seed(seed),\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      output_type=\"latent\"\n",
        "                  ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "              case isMode.on_controlnet:\n",
        "                  latents = self.pipe(\n",
        "                      prompt=prompt,\n",
        "                      image=source_image,\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      generator=torch.manual_seed(seed),\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      # max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      output_type=\"latent\"\n",
        "                  ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "              # case _: # デフォルト処理として on_t2i を選択します\n",
        "              #     latents = self.pipe(\n",
        "              #         prompt=prompt,\n",
        "              #         negative_prompt=negative_prompt,\n",
        "              #         num_inference_steps=num_inference_steps,\n",
        "              #         generator=torch.manual_seed(seed),\n",
        "              #         guidance_scale=guidance_scale,\n",
        "              #         max_embeddings_multiples=max_embeddings_multiples,\n",
        "              #         width=width,\n",
        "              #         height=height,\n",
        "              #         output_type=\"latent\"\n",
        "              #     ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "          if save_path is not None:\n",
        "              pil_image = self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "              os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "              pil_image.save(save_path, quality=95)\n",
        "\n",
        "          if output_type == \"latent\":\n",
        "              return latents\n",
        "          elif output_type == \"pil\":\n",
        "              return self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "          else:\n",
        "              raise NotImplementedError()\n",
        "\n",
        "  def diffusion_from_image(\n",
        "          self,\n",
        "          prompt,\n",
        "          negative_prompt,\n",
        "          image,\n",
        "          scheduler_name=\"dpm++_2m_karras\",\n",
        "          num_inference_steps=20,\n",
        "          denoising_strength=0.58,\n",
        "          guidance_scale=10,\n",
        "          output_type=\"pil\",\n",
        "          decode_factor=0.18215,\n",
        "          seed=1234,\n",
        "          max_embeddings_multiples=None,\n",
        "          save_path=None\n",
        "          ):\n",
        "\n",
        "      self.pipe_i2i.scheduler = SCHEDULERS[scheduler_name].from_config(self.pipe_i2i.scheduler.config)\n",
        "      self.pipe_i2i.scheduler.set_timesteps(num_inference_steps, self.device)\n",
        "      seed = random.randint(1, 1000000000) if seed == -1 else seed\n",
        "\n",
        "      with torch.no_grad():\n",
        "          latents = self.pipe_i2i(\n",
        "              prompt=prompt,\n",
        "              negative_prompt=negative_prompt,\n",
        "              image=image,\n",
        "              num_inference_steps=num_inference_steps,\n",
        "              strength=denoising_strength,\n",
        "              generator=torch.manual_seed(seed),\n",
        "              max_embeddings_multiples=max_embeddings_multiples,\n",
        "              guidance_scale=guidance_scale,\n",
        "              output_type=\"latent\"\n",
        "          ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "          if save_path is not None:\n",
        "              pil_image = self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "              os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "              pil_image.save(save_path, quality=95)\n",
        "\n",
        "          if output_type == \"latent\":\n",
        "              return latents\n",
        "          elif output_type == \"pil\":\n",
        "              return self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "          else:\n",
        "              raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def diffusion_enhance(\n",
        "          self,\n",
        "          prompt,\n",
        "          source_image,\n",
        "          negative_prompt,\n",
        "          scheduler_name=\"dpm++_2m_karras\",\n",
        "          num_inference_steps=20,\n",
        "          num_inference_steps_enhance=20,\n",
        "          guidance_scale=10,\n",
        "          width=512,\n",
        "          height=512,\n",
        "          seed=1234,\n",
        "          max_embeddings_multiples=None,\n",
        "          upscale_target=\"latent\", # \"latent\" or \"pil\"\n",
        "          interpolate_mode=\"nearest\",\n",
        "          antialias = True,\n",
        "          upscale_by=1.8,\n",
        "          enhance_steps=2, # 2=Hires.fix\n",
        "          denoising_strength=0.58,\n",
        "          output_type=\"pil\",\n",
        "          decode_factor=0.15,\n",
        "          decode_factor_final=0.18215,\n",
        "          save_dir=\"output\"\n",
        "          ):\n",
        "\n",
        "      with torch.no_grad():\n",
        "          w_init = calc_pix_8(width)\n",
        "          h_init = calc_pix_8(height)\n",
        "          w_final = calc_pix_8(w_init * upscale_by)\n",
        "          h_final = calc_pix_8(h_init * upscale_by)\n",
        "          resolution_pairs = [(calc_pix_8(x), calc_pix_8(y)) for x, y\n",
        "                  in zip(np.linspace(w_init, w_final, enhance_steps),\n",
        "                          np.linspace(h_init, h_final, enhance_steps))\n",
        "                  ]\n",
        "          image = None\n",
        "          now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "          if enhance_steps == 1: # Single generation\n",
        "              image = self.diffusion_from_noise(\n",
        "                      prompt,\n",
        "                      source_image,\n",
        "                      negative_prompt,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      width=w_final,\n",
        "                      height=h_final,\n",
        "                      output_type=output_type,\n",
        "                      decode_factor=decode_factor_final,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}.jpg\")\n",
        "                  )\n",
        "              return image\n",
        "\n",
        "\n",
        "          for i, (w, h) in enumerate(resolution_pairs):\n",
        "\n",
        "              if image is None: # Step 1: Generate low-quality image\n",
        "                  image = self.diffusion_from_noise(\n",
        "                      prompt,\n",
        "                      source_image,\n",
        "                      negative_prompt,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      width=w,\n",
        "                      height=h,\n",
        "                      output_type=upscale_target,\n",
        "                      decode_factor=decode_factor,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}_{i}.jpg\")\n",
        "                  )\n",
        "                  continue\n",
        "\n",
        "              # Step 2: Interpolate latent or image -> PIL image\n",
        "              if upscale_target == \"latent\":\n",
        "                  image = torch.nn.functional.interpolate(\n",
        "                          image,\n",
        "                          (h // 8, w // 8),\n",
        "                          mode=interpolate_mode,\n",
        "                          antialias=True if antialias and interpolate_mode != \"nearest\" else False,\n",
        "                      )\n",
        "                  image = self.decode_latents_to_PIL_image(image, decode_factor)\n",
        "              else:\n",
        "                  image = image.resize((w, h), Image.Resampling.LANCZOS)\n",
        "\n",
        "              # Step 3: Generate image (i2i)\n",
        "              if i < len(resolution_pairs) - 1:\n",
        "                  image = self.diffusion_from_image(\n",
        "                      prompt,\n",
        "                      negative_prompt,\n",
        "                      image,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=int(num_inference_steps_enhance / denoising_strength) + 1,\n",
        "                      denoising_strength=denoising_strength,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      output_type=upscale_target,\n",
        "                      decode_factor=decode_factor,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}_{i}.jpg\")\n",
        "                  )\n",
        "\n",
        "              else: # Final enhance\n",
        "                  image = self.diffusion_from_image(\n",
        "                      prompt,\n",
        "                      negative_prompt,\n",
        "                      image,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=int(num_inference_steps_enhance / denoising_strength) + 1,\n",
        "                      denoising_strength=denoising_strength,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      output_type=output_type,\n",
        "                      decode_factor=decode_factor_final,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}_{i}.jpg\")\n",
        "                  )\n",
        "                  return image\n",
        "\n",
        "  %cd /content/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @markdown ***\n",
        "# @markdown  i2i, controlnet に使用する画像を設定\n",
        "\n",
        "# 画像ファイルを取得する関数\n",
        "def get_jpeg_files(source_dir, extension):\n",
        "    files = [] # ファイル名のリストを初期化\n",
        "    for ext in extension: # 拡張子ごとにループ\n",
        "        images_path = os.path.join(source_dir, ext)\n",
        "        files.extend(glob.glob(images_path)) # パターンにマッチするファイル名をリストに追加\n",
        "    return files\n",
        "\n",
        "\n",
        "# 画像またはポーズのリストを作成する関数\n",
        "def get_images(jpeg_files, mode):\n",
        "    images = []\n",
        "    if mode == \"i2i\":\n",
        "        for file in jpeg_files:\n",
        "            # ファイルパスから画像を読み込んでリストに追加する\n",
        "            p = Image.open(file)\n",
        "            images.append(p)\n",
        "    elif mode == \"openpose\":\n",
        "        # ポーズを検出する\n",
        "        controlnet_check_path = \"lllyasviel/ControlNet\"\n",
        "        pose_detector = OpenposeDetector.from_pretrained(controlnet_check_path) # 事前学習済みのモデルを読み込む\n",
        "        for file in jpeg_files:\n",
        "            # ファイルパスから画像を読み込んでポーズを検出する\n",
        "            p = pose_detector(Image.open(file))\n",
        "            # ポーズをリストに追加する\n",
        "            images.append(p)\n",
        "    elif mode == \"canny\":\n",
        "        for file in jpeg_files:\n",
        "            # リスト内のファイル名を順番に処理する\n",
        "            image = load_image(file)\n",
        "            image = np.array(image)\n",
        "            # 画像のデータ型をCV_8Uに変換する\n",
        "            low_threshold, high_threshold = 100, 200\n",
        "            image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "            image = image[:, :, None]\n",
        "            image = np.concatenate([image, image, image], axis=2)\n",
        "            canny_image = Image.fromarray(image)\n",
        "            canny_image\n",
        "            # ポーズをリストに追加する\n",
        "            images.append(canny_image)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "    return images\n",
        "\n",
        "\n",
        "# コントロールネットワークのモデルパスを取得する関数\n",
        "def get_controlnet_model_path(mode):\n",
        "    if mode == \"openpose\":\n",
        "        return \"lllyasviel/sd-controlnet-openpose\"\n",
        "    elif mode == \"canny\":\n",
        "        return \"lllyasviel/sd-controlnet-canny\"\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "\n",
        "\n",
        "# 事前学習済みのモデルを読み込む関数\n",
        "def load_model(model_path):\n",
        "    # データ型はfloat16に指定する\n",
        "    return ControlNetModel.from_pretrained(model_path, torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "if isMode.on_i2i or isMode.on_controlnet:\n",
        "    # ファイルのパスのリストを取得\n",
        "    source_dir = \"/content/drive/MyDrive/StableDiffusion/ImageSource/wedding\" #@param {type: \"string\"}\n",
        "    extension = \"*.jpg\", \"*.png\", \"*.jpeg\", \"*.webp\" # 複数の拡張子をタプルで指定\n",
        "    jpeg_files = get_jpeg_files(source_dir, extension)\n",
        "\n",
        "    # @markdown ***\n",
        "    # @markdown ***Controlnet を使う場合、どれかを選ぶ***\n",
        "    is_openpose = False # @param {type:\"boolean\"}\n",
        "    is_canny = True # @param {type:\"boolean\"}\n",
        "\n",
        "    # for_generate_images = []\n",
        "    # match-case文で条件分岐する\n",
        "    match True:\n",
        "        case isMode.on_i2i:\n",
        "            # 画像のリストを作成する\n",
        "            for_generate_images = get_images(jpeg_files, \"i2i\")\n",
        "            # i2i_images = get_images(jpeg_files, \"i2i\")\n",
        "        case isMode.on_controlnet:\n",
        "            # ポーズのリストを作成する\n",
        "            for_generate_images = get_images(jpeg_files, \"openpose\" if is_openpose else \"canny\")\n",
        "            # controlnet_images = get_images(jpeg_files, \"openpose\" if is_openpose else \"canny\")\n",
        "            if len(for_generate_images) == 0:\n",
        "                raise ValueError(\"posesが空です\")\n",
        "            # コントロールネットワークのモデルパスを取得する\n",
        "            controlnet_model_path = get_controlnet_model_path(\"openpose\" if is_openpose else \"canny\")\n",
        "            # 事前学習済みのモデルを読み込む\n",
        "            controlnet = load_model(controlnet_model_path)\n",
        "        # case _:\n",
        "        #     rraise ValueError(\"Invalid condition\")\n",
        "\n",
        "\n",
        "# @markdown ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ9NqU4F8UpJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Model、LoRA を StableDiffusionImageGeneratorMod に設定 { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "#モデル\n",
        "# @markdown ***\n",
        "# @markdown 👚 Model\n",
        "model_dir = \"/content/drive/MyDrive/StableDiffusion/Model/\"\n",
        "model_name = \"beautifulRealistic_v60\" #@param [\"kanpiromix_v20\",\"chilled_remix_v2\", \"emilianJR/XXMix_9realistic\", \"beautifulRealistic_v60\", \"sinkinai/Beautiful-Realistic-Asians-v5\", \"emilianJR/majicMIX_realistic_v6\", \"runwayml/stable-diffusion-v1-5\", \"stablediffusionapi/anything-v5\"] {allow-input: true}\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "# 「/」が含まれていたら、〇〇を実行する\n",
        "if \"/\" in model_name:\n",
        "  model_path = model_name\n",
        "\n",
        "# VAE を読み込む\n",
        "vae_path = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\",\"stabilityai/sd-vae-ft-ema\", \"rossiyareich/anything-v4.0-vae\"] {allow-input: true}\n",
        "vae=AutoencoderKL.from_pretrained(pretrained_model_name_or_path=vae_path, torch_dtype=torch.float16)\n",
        "\n",
        "# パイプラインを構築\n",
        "generator = StableDiffusionImageGeneratorMod(\n",
        "  model_path,\n",
        "  controlnet=controlnet if isMode.on_controlnet else None, # 条件式を使って引数の値を決める\n",
        "  vae=vae,\n",
        ")\n",
        "\n",
        "# @markdown ***\n",
        "# @markdown 🙎 LoRA  →  ファイル名に  .safetensors  をつけること。\n",
        "lora_base_dir = \"/content/drive/MyDrive/StableDiffusion/Lora/\"\n",
        "\n",
        "isUse_lora_1 = True #@param {type:\"boolean\"}\n",
        "if isUse_lora_1:\n",
        "  lora_1=\"flat2.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_1 = -1 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_1 = os.path.join(lora_base_dir, lora_1)\n",
        "  generator.load_lora(lora_path_1, alpha=lora_alpha_1)\n",
        "\n",
        "isUse_lora_2= True #@param {type:\"boolean\"}\n",
        "if isUse_lora_2:\n",
        "  lora_2=\"haruchanAI_realitic.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_2 = 0.4 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_2 = os.path.join(lora_base_dir, lora_2)\n",
        "  generator.load_lora(lora_path_2, alpha=lora_alpha_2)\n",
        "\n",
        "isUse_lora_3= False #@param {type:\"boolean\"}\n",
        "if isUse_lora_3:\n",
        "  lora_3=\"WeddingDressEXv0.4.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_3 = 0.4 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_3 = os.path.join(lora_base_dir, lora_3)\n",
        "  generator.load_lora(lora_path_3, alpha=lora_alpha_3)\n",
        "\n",
        "isUse_lora_4= True #@param {type:\"boolean\"}\n",
        "if isUse_lora_4:\n",
        "  lora_4=\"koreanDollLikeness_v20.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_4 = 0.5 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_4 = os.path.join(lora_base_dir, lora_4)\n",
        "  generator.load_lora(lora_path_4, alpha=lora_alpha_4)\n",
        "\n",
        "isUse_lora_5= True #@param {type:\"boolean\"}\n",
        "if isUse_lora_5:\n",
        "  lora_5=\"japaneseDollLikeness_v15.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_5 = 0.5 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_5 = os.path.join(lora_base_dir, lora_5)\n",
        "  generator.load_lora(lora_path_5, alpha=lora_alpha_5)\n",
        "\n",
        "# @markdown ***\n",
        "#@markdown 👾 Embeddings\n",
        "embeddings_base_dir = \"/content/drive/MyDrive/StableDiffusion/embeddings/\"\n",
        "\n",
        "isUse_embedding_1 = True #@param {type:\"boolean\"}\n",
        "if isUse_embedding_1:\n",
        "  embedding_1 = \"EasyNegative.safetensors\" #@param {type:\"string\"}\n",
        "  trigger_word_1 = \"EasyNegative\" #@param {type:\"string\"}\n",
        "  safetensors_path_1 = os.path.join(embeddings_base_dir, embedding_1)\n",
        "  generator.load_embeddings(safetensors_path_1, embedding_1, trigger_word_1)\n",
        "\n",
        "isUse_embedding_2 = True #@param {type:\"boolean\"}\n",
        "if isUse_embedding_2:\n",
        "  embedding_2 = \"negative_hand-neg.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_2 = \"negative_hand-neg\" #@param {type:\"string\"}\n",
        "  safetensors_path_2 = os.path.join(embeddings_base_dir, embedding_2)\n",
        "  generator.load_embeddings(safetensors_path_2, embedding_2, trigger_word_2)\n",
        "\n",
        "isUse_embedding_3 = False #@param {type:\"boolean\"}\n",
        "if isUse_embedding_3:\n",
        "  embedding_3 = \"ulzzang-6500-v1.1.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_3 = \"ulzzang-6500-v1.1\" #@param {type:\"string\"}\n",
        "  safetensors_path_3 = os.path.join(embeddings_base_dir, embedding_3)\n",
        "  generator.load_embeddings(safetensors_path_3, embedding_3, trigger_word_3)\n",
        "\n",
        "\n",
        "isUse_embedding_4 = True #@param {type:\"boolean\"}\n",
        "if isUse_embedding_4:\n",
        "  embedding_4 = \"ng_deepnegative_v1_75t.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_4 = \"ng_deepnegative_v1_75t\" #@param {type:\"string\"}\n",
        "  safetensors_path_4 = os.path.join(embeddings_base_dir, embedding_4)\n",
        "  generator.load_embeddings(safetensors_path_4, embedding_4, trigger_word_4)\n",
        "\n",
        "\n",
        "isUse_embedding_5 = False #@param {type:\"boolean\"}\n",
        "if isUse_embedding_5:\n",
        "  embedding_5 = \"GS-DeFeminize-neg.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_5 = \"GS-Masculine, GS-DeFeminize-Neg\" #@param {type:\"string\"}\n",
        "  safetensors_path_5 = os.path.join(embeddings_base_dir, embedding_5)\n",
        "  generator.load_embeddings(safetensors_path_5, embedding_4, trigger_word_5)\n",
        "\n",
        "\n",
        "# @markdown ***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYM1Dv3QeR08",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title 画像を生成  { display-mode: \"form\" }\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "# ファイル名に使う日付と時刻のフォーマットを定義する\n",
        "file_format = \"%Y%m%d_%H%M%S\"\n",
        "i=0\n",
        "\n",
        "# 現在の日本時間を取得\n",
        "jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "#txt2img出力画像の保存先\n",
        "#@markdown ***出力画像を保存するフォルダ***\n",
        "save_dir = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/\"\n",
        "save_folder = \"test-wedding\" #@param {type: \"string\"}\n",
        "save_path = os.path.join(save_dir, save_folder)\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "#@@markdown ポジティブプロンプト\n",
        "prompt = \"((masterpiece)), ((super detailed)), photo-realistic, ((best quality)), 1girl, embarrassed face, small Breast, ((standing, full body shot)), (wedding dress, veil:1.4), in beach, SunLight\" #@param {type:\"string\"}\n",
        "#@@markdown ネガティブプロンプト\n",
        "negative = \"((negative_hand)), ((negative_hand-neg)), ((ng_deepnegative_v1_75t)), EasyNegative, Simple background:1.5, ((worst picture quality, low quality, normal quality)),  painting, sketch, text, error, jpeg image, logo, signature, watermark, Username, (extra fingers, deformed hands, polydactyl:1.5), long neck, bad proportions\" #@param {type:\"string\"}\n",
        "# 使用する画像\n",
        "if isMode.on_controlnet or isMode.on_i2i:\n",
        "    #@markdown ***🏞️ 画像 → ランダムに使うか？ ◯番目をつかうか？***\n",
        "    is_random_image = True #@param {type: \"boolean\"}\n",
        "    image_index = 0 # @param {type:\"slider\", min:0, max:20, step:1}\n",
        "    if image_index <= 0 or image_index > len(for_generate_images):\n",
        "      image_index = 0\n",
        "    else:\n",
        "      image_index -= 1\n",
        "\n",
        "\n",
        "#@markdown ***出力枚数***\n",
        "batch_count = 2 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "#@markdown ***ステップ数***\n",
        "steps = 5 # @param {type:\"slider\", min:5, max:100, step:5}\n",
        "#@@markdown 画像サイズ\n",
        "img_width =   512  # @param {type:\"slider\", min:256, max:1920, step:64}\n",
        "img_height =  768  # @param {type:\"slider\", min:256, max:1920, step:64}\n",
        "#@@markdown CFG\n",
        "CFG = 7  # @param {type:\"slider\", min:0, max:25, step:1}\n",
        "#@markdown ***スケジューラ(サンプラー)***\n",
        "scheduler=\"dpm++_2m_karras\" #@param [\"unipc\",\"euler_a\",\"euler\",\"ddim\",\"ddpm\",\"deis\",\"dpm2\",\"dpm++_2s\",\"dpm++_2m\",\"dpm++_2m_karras\",\"dpm++_sde\",\"dpm++_sde_karras\",\"heun\",\"heun_karras\",\"lms\",\"lms_karras\",\"pndm\",\"dpm++_2m_karras\"]\n",
        "#@@markdown シード（-1の時はランダム）\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "if seed is None or seed == -1:\n",
        "  inputSeed = random.randint(0, 2147483647)\n",
        "else:\n",
        "  valueSeed = seed\n",
        "#@markdown ***プロンプトのトークン制限をどのくらい緩和するか → 75 × N になる***\n",
        "max_embeddings_multiples=6  # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "#@markdown ***Hires.fix の有効化***\n",
        "hires_fix = True #@param {type: \"boolean\"}\n",
        "enhance_steps = 2 if hires_fix else 1\n",
        "#@markdown ***解像度倍率(乗算後、最も近い8の倍数のサイズとなる)***\n",
        "upscaling_ratio = 2 # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "if hires_fix == False:\n",
        "  upscaling_ratio=1\n",
        "#@markdown ***アップスケールステップ数***\n",
        "is_test_hires_fix = False #@param {type: \"boolean\"}\n",
        "up_steps = 5 # @param {type:\"slider\", min:5, max:100, step:5}\n",
        "if is_test_hires_fix == True:\n",
        "  up_steps = 10\n",
        "#@markdown ***小さいほど元画像を尊重） （0.5～0.7）が推奨***\n",
        "denoising_strength = 0.6 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "\n",
        "\n",
        "# 画像生成\n",
        "for i in range(batch_count):\n",
        "  if seed is None or seed == -1:valueSeed = inputSeed + i\n",
        "  if isMode.on_controlnet or isMode.on_i2i:\n",
        "      if is_random_image and len(for_generate_images) > 0:\n",
        "          image_index = random.randint(0, len(for_generate_images)-1)\n",
        "          source_image = for_generate_images[image_index]\n",
        "\n",
        "  image = generator.diffusion_enhance(\n",
        "    prompt,\n",
        "    source_image=source_image if source_image else None,\n",
        "    negative_prompt=negative,\n",
        "    scheduler_name=scheduler,\n",
        "    num_inference_steps=steps,\n",
        "    num_inference_steps_enhance=up_steps,\n",
        "    guidance_scale=CFG,\n",
        "    width=img_width,\n",
        "    height=img_height,\n",
        "    seed=valueSeed,\n",
        "    max_embeddings_multiples=max_embeddings_multiples,\n",
        "    upscale_target=\"latent\",\n",
        "    interpolate_mode=\"bicubic\",\n",
        "    antialias=True,\n",
        "    upscale_by=upscaling_ratio,\n",
        "    enhance_steps=enhance_steps,\n",
        "    denoising_strength=denoising_strength,\n",
        "    output_type=\"pil\",\n",
        "    decode_factor=0.15,\n",
        "    decode_factor_final=0.18215,\n",
        "  )\n",
        "\n",
        "  # 画像を保存\n",
        "\n",
        "  # 現在の日本時間を取得\n",
        "  jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  #出力する画像の名前を生成する\n",
        "  file_name = (jst_now.strftime(file_format)+ \"_\" + str(valueSeed))\n",
        "  image_name = file_name + f\".png\"\n",
        "\n",
        "  #画像を保存する\n",
        "  save_location = os.path.join(save_path, image_name)\n",
        "\n",
        "  #@markdown ***メタデータの書き込み***\n",
        "  save_metadata = True #@param {type: \"boolean\"}\n",
        "  if save_metadata:\n",
        "    metadata = PngInfo()\n",
        "    # Model\n",
        "    metadata.add_text(\"model\",(model_name))\n",
        "    # Lora\n",
        "    if isUse_lora_1:\n",
        "      metadata.add_text(\"lora_1\",(lora_1))\n",
        "      metadata.add_text(\"lora_alpha_1\",(str(lora_alpha_1)))\n",
        "    if isUse_lora_2:\n",
        "      metadata.add_text(\"lora_2\",(lora_2))\n",
        "      metadata.add_text(\"lora_alpha_2\",(str(lora_alpha_2)))\n",
        "    if isUse_lora_3:\n",
        "      metadata.add_text(\"lora_3\",(lora_3))\n",
        "      metadata.add_text(\"lora_alpha_3\",(str(lora_alpha_3)))\n",
        "    if isUse_lora_4:\n",
        "      metadata.add_text(\"lora_4\",(lora_4))\n",
        "      metadata.add_text(\"lora_alpha_4\",(str(lora_alpha_4)))\n",
        "    if isUse_lora_5:\n",
        "      metadata.add_text(\"lora_5\",(lora_5))\n",
        "      metadata.add_text(\"lora_alpha_5\",(str(lora_alpha_5)))\n",
        "    # embedding\n",
        "    if isUse_embedding_1:\n",
        "      metadata.add_text(\"embedding_1\",(embedding_1))\n",
        "      metadata.add_text(\"trigger_word_1\",(str(trigger_word_1)))\n",
        "    if isUse_embedding_2:\n",
        "      metadata.add_text(\"embedding_2\",(embedding_2))\n",
        "      metadata.add_text(\"trigger_word_2\",(str(trigger_word_2)))\n",
        "    if isUse_embedding_3:\n",
        "      metadata.add_text(\"embedding_3\",(embedding_3))\n",
        "      metadata.add_text(\"trigger_word_3\",(str(trigger_word_3)))\n",
        "    if isUse_embedding_4:\n",
        "      metadata.add_text(\"embedding_4\",(embedding_4))\n",
        "      metadata.add_text(\"trigger_word_4\",(str(trigger_word_4)))\n",
        "    if isUse_embedding_5:\n",
        "      metadata.add_text(\"embedding_5\",(embedding_5))\n",
        "      metadata.add_text(\"trigger_word_5\",(str(trigger_word_5)))\n",
        "    # プロンプト\n",
        "    metadata.add_text(\"prompt\",(prompt))\n",
        "    metadata.add_text(\"negative\",(negative))\n",
        "    metadata.add_text(\"scheduler\",(scheduler))\n",
        "    metadata.add_text(\"steps\",(str(steps)))\n",
        "    metadata.add_text(\"CFG\",(str(CFG)))\n",
        "    metadata.add_text(\"width\",(str(img_width)))\n",
        "    metadata.add_text(\"height\",(str(img_height)))\n",
        "    metadata.add_text(\"seed\",str((valueSeed)))\n",
        "  if hires_fix:\n",
        "    metadata.add_text(\"upscaling ratio\",str((upscaling_ratio)))\n",
        "    metadata.add_text(\"up steps\",str((up_steps)))\n",
        "    metadata.add_text(\"denoising strength\",str((denoising_strength)))\n",
        "    image.save(save_location, pnginfo=metadata)\n",
        "  else:\n",
        "    image.save(save_location)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kbLsCgi4Uyzo"
      },
      "outputs": [],
      "source": [
        "# @title 画像のメタデータを出力\n",
        "# Driveに接続\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "isConectDrive=False  #@param {type: \"boolean\"}\n",
        "if isConectDrive:\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#@markdown **保存した画像のパス**\n",
        "file_dir = \"/content/drive/MyDrive/StableDiffusion/txt2img_output\"  #@param {type: \"string\"}\n",
        "file_name = \"20230726_195024_606196166.png\" #@param {type: \"string\"}\n",
        "file_path = os.path.join(file_dir, file_name)\n",
        "if file_path:\n",
        "  img = Image.open(file_path)\n",
        "\n",
        "  # Model\n",
        "  if \"model\" in img.text:\n",
        "    print(\"model: \",img.text[\"model\"])\n",
        "  # LoRA\n",
        "  if \"lora_1\" in img.text:\n",
        "    print(\"lora_1: \",img.text[\"lora_1\"])\n",
        "    print(\"lora_alpha_1: \", img.text[\"lora_alpha_1\"])\n",
        "  if \"lora_2\" in img.text:\n",
        "    print(\"lora_2: \",img.text[\"lora_2\"])\n",
        "    print(\"lora_alpha_2: \", img.text[\"lora_alpha_2\"])\n",
        "  if \"lora_3\" in img.text:\n",
        "    print(\"lora_3: \",img.text[\"lora_3\"])\n",
        "    print(\"lora_alpha_3: \", img.text[\"lora_alpha_3\"])\n",
        "  if \"lora_4\" in img.text:\n",
        "    print(\"lora_4: \",img.text[\"lora_4\"])\n",
        "    print(\"lora_alpha_4: \", img.text[\"lora_alpha_4\"])\n",
        "  if \"lora_5\" in img.text:\n",
        "    print(\"lora_5: \",img.text[\"lora_5\"])\n",
        "    print(\"lora_alpha_5: \", img.text[\"lora_alpha_5\"])\n",
        "  # embedding\n",
        "  if \"embedding_1\" in img.text:\n",
        "    print(\"embedding_1: \",img.text[\"embedding_1\"])\n",
        "    print(\"trigger_word_1: \", img.text[\"trigger_word_1\"])\n",
        "  if \"embedding_2\" in img.text:\n",
        "    print(\"embedding_2: \",img.text[\"embedding_2\"])\n",
        "    print(\"trigger_word_2: \", img.text[\"trigger_word_2\"])\n",
        "  if \"embedding_3\" in img.text:\n",
        "    print(\"embedding_3: \",img.text[\"embedding_3\"])\n",
        "    print(\"trigger_word_3: \", img.text[\"trigger_word_3\"])\n",
        "  if \"embedding_4\" in img.text:\n",
        "    print(\"embedding_4: \",img.text[\"embedding_4\"])\n",
        "    print(\"trigger_word_4: \", img.text[\"trigger_word_4\"])\n",
        "  if \"embedding_5\" in img.text:\n",
        "    print(\"embedding_5: \",img.text[\"embedding_5\"])\n",
        "    print(\"trigger_word_5: \", img.text[\"trigger_word_5\"])\n",
        "  # Prompt\n",
        "  print(\"Prompt: \",img.text[\"prompt\"])\n",
        "  print(\"Negative Prompt: \",img.text[\"negative\"])\n",
        "  if \"scheduler\" in img.text:\n",
        "    print(\"Scheduler: \", img.text[\"scheduler\"])\n",
        "  print(\"Steps: \",img.text[\"steps\"])\n",
        "  print(\"CFG: \",img.text[\"CFG\"])\n",
        "  print(\"Width: \",img.text[\"width\"])\n",
        "  print(\"Height: \",img.text[\"height\"])\n",
        "  print(\"Seed: \",img.text[\"seed\"])\n",
        "  # Hires.fix\n",
        "  try:\n",
        "    print(\"Upscaling ratio; \",img.text[\"upscaling ratio\"])\n",
        "    print(\"Up steps: \",img.text[\"up steps\"])\n",
        "    print(\"Denoising strength: \",img.text[\"denoising strength\"])\n",
        "  except:\n",
        "    print(\"Hires.fix was OFF.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ila9x3rQUyzo"
      },
      "outputs": [],
      "source": [
        "# 画像処理用のライブラリをインポート\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# globモジュールをインポート\n",
        "import glob\n",
        "\n",
        "# natsortをインポートする\n",
        "from natsort import natsorted\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "isConectDrive=False  #@param {type: \"boolean\"}\n",
        "if isConectDrive:\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "# 画像ファイルのパスをリストに格納\n",
        "# ここでは例として、フォルダ名がimagesで、PNG画像をすべて指定する\n",
        "path = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/grid/*\" #@param {type:\"string\"}\n",
        "image_paths = []\n",
        "for ext in [\"png\", \"jpeg\", \"jpg\"]:\n",
        "  image_paths.extend(glob.glob(path + \".\" + ext))\n",
        "\n",
        "\n",
        "if image_paths:\n",
        "    # リストをファイル名で自然順にソートする\n",
        "    image_paths = natsorted(image_paths)\n",
        "\n",
        "    # 画像の高さの最小値と幅の最大値を初期化する\n",
        "    min_height = float(\"inf\")\n",
        "    # min_height = 0 # 最小値を0に設定\n",
        "    max_width = 0\n",
        "\n",
        "    # image_pathsの画像の中で一番高さの低い画像の高さと一番幅の大きい画像の幅を取得する\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path) # 画像を読み込む\n",
        "        h, w, c = img.shape # 画像の高さ、幅、チャンネル数を取得する\n",
        "        if h < min_height: # もし高さが最小値より小さい場合は\n",
        "            min_height = h # 最小値を更新する\n",
        "        if w > max_width: # もし幅が最大値より大きい場合は\n",
        "            max_width = w # 最大値を更新する\n",
        "\n",
        "\n",
        "    isAddBlack = False #@param {type: \"boolean\"}\n",
        "\n",
        "    # 画像ファイルを読み込んで、リサイズしてリストに格納\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path) # 画像を読み込む\n",
        "        h, w, c = img.shape # 画像の高さ、幅、チャンネル数を取得する\n",
        "        scale = min_height / h # 縮小率を計算する\n",
        "        new_h = min_height # 新しい高さは最小値とする\n",
        "        new_w = int(w * scale) # 新しい幅を計算する（整数型に変換）\n",
        "        img = cv2.resize(img, (new_w, new_h)) # 画像をリサイズする\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGRからRGBに色空間を変換する\n",
        "\n",
        "        # # もし幅が最大値より小さい場合は、右側に黒い余白を追加する\n",
        "        # if new_w < max_width:\n",
        "        #     pad_width = max_width - new_w # 余白の幅を計算する（整数型に変換）\n",
        "        #     pad_img = np.zeros((new_h, pad_width, 3), dtype=np.uint8) # 黒い余白の画像を作る\n",
        "        #     img = np.hstack((img, pad_img)) # 元の画像と余白の画像を水平方向に結合する\n",
        "\n",
        "        if isAddBlack:\n",
        "          # もし幅が最大値より小さい場合は、左右に黒い余白を追加する\n",
        "          if new_w < max_width:\n",
        "              pad_width = max_width - new_w # 余白の幅を計算する（整数型に変換）\n",
        "              pad_left = pad_width // 2 # 左側の余白の幅を計算する（整数型に変換）\n",
        "              pad_right = pad_width - pad_left # 右側の余白の幅を計算する（整数型に変換）\n",
        "              pad_img_left = np.zeros((new_h, pad_left, 3), dtype=np.uint8) # 左側の黒い余白の画像を作る\n",
        "              pad_img_right = np.zeros((new_h, pad_right, 3), dtype=np.uint8) # 右側の黒い余白の画像を作る\n",
        "              img = np.hstack((pad_img_left, img, pad_img_right)) # 左側の余白、元の画像、右側の余白を水平方向に結合する\n",
        "\n",
        "        images.append(img) # リストに追加する\n",
        "\n",
        "\n",
        "    # 画像をグリッド表示するためのパラメータを設定\n",
        "    # 列数\n",
        "    rows = 2 #@param {type:\"integer\"}\n",
        "    # 行数\n",
        "    cols = 2 #@param {type:\"integer\"}\n",
        "    required_images = 4 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "    # 画像が足りない場合は、黒い画像を作ってリストに追加する\n",
        "    num_images = len(images) # リストに格納された画像の数\n",
        "    if num_images < required_images: # もし足りない場合は\n",
        "        for i in range(required_images - num_images): # 足りない分だけ繰り返す（整数型に変換）\n",
        "            black_img = np.zeros((int(min_height), int(max_width), 3), dtype=np.uint8) # 黒い画像を作る（整数型に変換）\n",
        "            images.append(black_img) # リストに追加する\n",
        "\n",
        "    # 画像をグリッド表示するための関数を定義\n",
        "    def grid_display(images, rows, cols):\n",
        "        # 画像の数と行列数が一致しない場合はエラーを出す\n",
        "        assert len(images) == rows*cols\n",
        "\n",
        "        # 行列数に応じて画像を結合する\n",
        "        # まずは行ごとに画像を水平方向に結合する\n",
        "        horizontal = []\n",
        "        for i in range(rows):\n",
        "            start_index = i*cols # 行の先頭の画像のインデックス\n",
        "            end_index = start_index + cols # 行の最後の画像のインデックス\n",
        "            horizontal.append(np.hstack(images[start_index:end_index])) # 水平方向に結合した画像をリストに追加する\n",
        "\n",
        "        # 次に、結合した行ごとの画像を垂直方向に結合する\n",
        "        vertical = np.vstack(horizontal) # 垂直方向に結合した画像\n",
        "\n",
        "        # 結果を返す\n",
        "        return vertical\n",
        "\n",
        "    # 関数を呼び出して、画像をグリッド表示する\n",
        "    result = grid_display(images, cols, rows) # 関数を呼び出す\n",
        "    plt.figure(figsize=(50,50)) # 図のサイズを設定する\n",
        "    plt.imshow(result) # 図に画像を表示する（補間なし）\n",
        "    plt.axis(\"off\") # 軸を非表示にする\n",
        "    plt.show() # 図を表示する\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart session\n",
        "#@markdown Executing this cell crashes session. It's intended.<br>You don't need to re-execute cells above.\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hcMkiFDQYzHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}