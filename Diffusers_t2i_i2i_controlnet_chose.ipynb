{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_t2i_i2i_controlnet_chose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDWXJo4TqbdN"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Diffusers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦ã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚**\n",
        "å‚è€ƒ ï¼š\n",
        "[æ—¥æœ¬èª](https://blog.shikoan.com/controlnet_lora/#ControlNet%EF%BC%88%E3%83%9D%E3%83%BC%E3%82%BA%EF%BC%89%E3%82%92%E4%BD%BF%E3%81%86)  ,\n",
        "[è‹±èª](https://huggingface.co/blog/controlnet) ,\n",
        "[Github - AA-google-colab-kohya](https://github.com/koya-jp/AA-google-colab-kohya/blob/master/Diffusers_S2D2.ipynb)\n",
        "\n",
        "[â€»ç¾æ™‚ç‚¹ã§ã€Controlnet ã¨ã¯ä½µç”¨ã§ããªã„ã€‚](https://github.com/keisuke-okb/S2D2/tree/main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2_jumxegTi0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9c52e677-edd8-4c64-c0dc-e2e0098476f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Driveã«æ¥ç¶š, ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¿½åŠ  { display-mode: \"form\" }\n",
        "\n",
        "# Driveã«æ¥ç¶š\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¿½åŠ \n",
        "# æœ€æ–°ç‰ˆï¼ˆ0.18.0ï¼‰\n",
        "!pip install git+https://github.com/huggingface/diffusers.git@v0.18.0 >/dev/null 2>&1\n",
        "!pip install --upgrade transformers accelerate scipy ftfy xformers safetensors txt2img >/dev/null 2>&1 # diffusers==0.17.1\n",
        "!pip install k-diffusion OmegaConf opencv-contrib-python controlnet_aux >/dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XesbQoV9Igeq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "collapsed": true,
        "outputId": "ea2d2c17-4382-4792-aa68-536f19617db7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title è‡ªå‹•åˆ‡æ–­ã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ã‚³ãƒ¼ãƒ‰ { display-mode: \"form\" }\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFwHNs_9xLCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d08b353f-c684-46b9-f047-687e27a8a160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/S2D2\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# @markdown ***\n",
        "#@title ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ + S2D2 / Embeddingsã®æº–å‚™ { display-mode: \"form\" }\n",
        "%cd /content\n",
        "!git clone https://github.com/keisuke-okb/S2D2 &> /dev/null\n",
        "\n",
        "%cd ./S2D2\n",
        "!git pull\n",
        "!touch __init__.py\n",
        "!pip install -r requirements.txt >/dev/null 2>&1\n",
        "\n",
        "\n",
        "# å„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import s2d2\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# diffusers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "import diffusers\n",
        "from diffusers import (StableDiffusionPipeline, StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler, StableDiffusionImg2ImgPipeline)\n",
        "from diffusers.utils import load_image, numpy_to_pil\n",
        "# from diffusers.utils import register_pipeline\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "from diffusers.models import AutoencoderKL\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# controlnet_auxã¨ã„ã†ãƒãƒ¼ã‚ºæ¤œå‡ºã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "# PyTorchã¨ã„ã†æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "import torch\n",
        "\n",
        "# PILã¨ã„ã†ç”»åƒå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "##################################################\n",
        "\n",
        "\n",
        "device=\"cuda\"\n",
        "\n",
        "\n",
        "SCHEDULERS = {\n",
        "    \"unipc\": diffusers.schedulers.UniPCMultistepScheduler,\n",
        "    \"euler_a\": diffusers.schedulers.EulerAncestralDiscreteScheduler,\n",
        "    \"euler\": diffusers.schedulers.EulerDiscreteScheduler,\n",
        "    \"ddim\": diffusers.schedulers.DDIMScheduler,\n",
        "    \"ddpm\": diffusers.schedulers.DDPMScheduler,\n",
        "    \"deis\": diffusers.schedulers.DEISMultistepScheduler,\n",
        "    \"dpm2\": diffusers.schedulers.KDPM2DiscreteScheduler,\n",
        "    \"dpm2-a\": diffusers.schedulers.KDPM2AncestralDiscreteScheduler,\n",
        "    \"dpm++_2s\": diffusers.schedulers.DPMSolverSinglestepScheduler,\n",
        "    \"dpm++_2m\": diffusers.schedulers.DPMSolverMultistepScheduler,\n",
        "    \"dpm++_2m_karras\": diffusers.schedulers.DPMSolverMultistepScheduler,\n",
        "    \"dpm++_sde\": diffusers.schedulers.DPMSolverSDEScheduler,\n",
        "    \"dpm++_sde_karras\": diffusers.schedulers.DPMSolverSDEScheduler,\n",
        "    \"heun\": diffusers.schedulers.HeunDiscreteScheduler,\n",
        "    \"heun_karras\": diffusers.schedulers.HeunDiscreteScheduler,\n",
        "    \"lms\": diffusers.schedulers.LMSDiscreteScheduler,\n",
        "    \"lms_karras\": diffusers.schedulers.LMSDiscreteScheduler,\n",
        "    \"pndm\": diffusers.schedulers.PNDMScheduler,\n",
        "}\n",
        "\n",
        "def calc_pix_8(x):\n",
        "    x = int(x)\n",
        "    return x - x % 8\n",
        "\n",
        "\n",
        "##################################################\n",
        "\n",
        "# @markdown ***t2i, i2i, controlnet ã®ã©ã‚Œã‚’ä½¿ç”¨ã™ã‚‹ã‹é¸ã¶***\n",
        "class isMode:\n",
        "    on_t2i = False #@param {type: \"boolean\"}\n",
        "    on_i2i = False #@param {type: \"boolean\"}\n",
        "    on_controlnet = True #@param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "class StableDiffusionImageGeneratorMod(s2d2.StableDiffusionImageGenerator):\n",
        "  def __init__(self, sd_safetensor_path: str, controlnet=None, vae=None, dtype: torch.dtype=torch.float16):\n",
        "\n",
        "    self.device = torch.device(device)\n",
        "    self.controlnet = controlnet\n",
        "    self.vae = vae\n",
        "\n",
        "    match True: # ã©ã‚Œã‹ä¸€ã¤ã® case ã«ãƒãƒƒãƒã•ã›ã‚‹ãŸã‚ã« True ã‚’ä½¿ã„ã¾ã™\n",
        "\n",
        "        case isMode.on_t2i:\n",
        "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                sd_safetensor_path,\n",
        "                torch_dtype=dtype,\n",
        "                vae=vae,\n",
        "                custom_pipeline=\"lpw_stable_diffusion\",\n",
        "            ).to(device)\n",
        "\n",
        "        case isMode.on_i2i:\n",
        "            self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "                sd_safetensor_path,\n",
        "                torch_dtype=dtype,\n",
        "                vae=vae,\n",
        "                custom_pipeline=\"lpw_stable_diffusion\",\n",
        "            ).to(device)\n",
        "\n",
        "        case isMode.on_controlnet:\n",
        "            self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "                sd_safetensor_path,\n",
        "                controlnet=controlnet,\n",
        "                torch_dtype=dtype,\n",
        "                vae=vae,\n",
        "                # custom_pipeline=\"lpw_stable_diffusion\", # â†’ ãªã„ã€‚\n",
        "            ).to(device)\n",
        "\n",
        "        # case _: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†ã¨ã—ã¦ on_t2i ã‚’é¸æŠã—ã¾ã™\n",
        "        #     self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        #         sd_safetensor_path,\n",
        "        #         torch_dtype=dtype,\n",
        "        #         vae=vae,\n",
        "        #         custom_pipeline=\"lpw_stable_diffusion\",\n",
        "        #     ).to(device)\n",
        "\n",
        "    # Hires.fix\n",
        "    self.pipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "      sd_safetensor_path,\n",
        "      vae=vae,\n",
        "      torch_dtype=dtype,\n",
        "      custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    ).to(device)\n",
        "\n",
        "    # NFSWã‚’ç„¡åŠ¹ã«ã™ã‚‹\n",
        "    self.pipe.safety_checker = None\n",
        "    self.pipe_i2i.safety_checker = None\n",
        "    # ç”»åƒç”Ÿæˆã‚’æ—©ãã™ã‚‹\n",
        "    self.pipe.enable_xformers_memory_efficient_attention()\n",
        "    self.pipe_i2i.enable_xformers_memory_efficient_attention()\n",
        "    return\n",
        "\n",
        "  def load_embeddings(self, safetensors_path: str, fileName: str, token: str):\n",
        "    if token != \"\":\n",
        "      self.pipe.load_textual_inversion(safetensors_path, weight_name=fileName, token=token)\n",
        "      self.pipe_i2i.load_textual_inversion(safetensors_path, weight_name=fileName, token=token)\n",
        "    else:\n",
        "      self.pipe.load_textual_inversion(safetensors_path, weight_name=fileName)\n",
        "      self.pipe_i2i.load_textual_inversion(safetensors_path, weight_name=fileName)\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def decode_latents_to_PIL_image(self, latents, decode_factor=0.18215):\n",
        "      with torch.no_grad():\n",
        "          latents = 1 / decode_factor * latents\n",
        "          image = self.pipe.vae.decode(latents).sample\n",
        "          image = (image / 2 + 0.5).clamp(0, 1)\n",
        "          image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "          image = numpy_to_pil(image)\n",
        "          image = StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None).images[0]\n",
        "          return image\n",
        "\n",
        "\n",
        "  def diffusion_from_noise(\n",
        "          self,\n",
        "          prompt,\n",
        "          source_image,\n",
        "          negative_prompt,\n",
        "          scheduler_name=\"dpm++_2m_karras\",\n",
        "          num_inference_steps=20,\n",
        "          guidance_scale=9.5,\n",
        "          width=512,\n",
        "          height=512,\n",
        "          output_type=\"pil\",\n",
        "          decode_factor=0.18215,\n",
        "          seed=1234,\n",
        "          max_embeddings_multiples=None,\n",
        "          save_path=None\n",
        "          ):\n",
        "\n",
        "      # # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’è¨­å®š\n",
        "      # pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "      self.pipe.scheduler = SCHEDULERS[scheduler_name].from_config(self.pipe.scheduler.config)\n",
        "      self.pipe.scheduler.set_timesteps(num_inference_steps, self.device)\n",
        "      seed = random.randint(1, 1000000000) if seed == -1 else seed\n",
        "\n",
        "      with torch.no_grad():\n",
        "          match True: # ã©ã‚Œã‹ä¸€ã¤ã® case ã«ãƒãƒƒãƒã•ã›ã‚‹ãŸã‚ã« True ã‚’ä½¿ã„ã¾ã™\n",
        "\n",
        "              case isMode.on_t2i:\n",
        "                  latents = self.pipe(\n",
        "                      prompt=prompt,\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      generator=torch.manual_seed(seed),\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      output_type=\"latent\"\n",
        "                  ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "              case isMode.on_i2i:\n",
        "                  latents = self.pipe(\n",
        "                      prompt=prompt,\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      image=source_image,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      generator=torch.manual_seed(seed),\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      output_type=\"latent\"\n",
        "                  ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "              case isMode.on_controlnet:\n",
        "                  latents = self.pipe(\n",
        "                      prompt=prompt,\n",
        "                      image=source_image,\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      generator=torch.manual_seed(seed),\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      # max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      output_type=\"latent\"\n",
        "                  ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "              # case _: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†ã¨ã—ã¦ on_t2i ã‚’é¸æŠã—ã¾ã™\n",
        "              #     latents = self.pipe(\n",
        "              #         prompt=prompt,\n",
        "              #         negative_prompt=negative_prompt,\n",
        "              #         num_inference_steps=num_inference_steps,\n",
        "              #         generator=torch.manual_seed(seed),\n",
        "              #         guidance_scale=guidance_scale,\n",
        "              #         max_embeddings_multiples=max_embeddings_multiples,\n",
        "              #         width=width,\n",
        "              #         height=height,\n",
        "              #         output_type=\"latent\"\n",
        "              #     ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "          if save_path is not None:\n",
        "              pil_image = self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "              os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "              pil_image.save(save_path, quality=95)\n",
        "\n",
        "          if output_type == \"latent\":\n",
        "              return latents\n",
        "          elif output_type == \"pil\":\n",
        "              return self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "          else:\n",
        "              raise NotImplementedError()\n",
        "\n",
        "  def diffusion_from_image(\n",
        "          self,\n",
        "          prompt,\n",
        "          negative_prompt,\n",
        "          image,\n",
        "          scheduler_name=\"dpm++_2m_karras\",\n",
        "          num_inference_steps=20,\n",
        "          denoising_strength=0.58,\n",
        "          guidance_scale=10,\n",
        "          output_type=\"pil\",\n",
        "          decode_factor=0.18215,\n",
        "          seed=1234,\n",
        "          max_embeddings_multiples=None,\n",
        "          save_path=None\n",
        "          ):\n",
        "\n",
        "      self.pipe_i2i.scheduler = SCHEDULERS[scheduler_name].from_config(self.pipe_i2i.scheduler.config)\n",
        "      self.pipe_i2i.scheduler.set_timesteps(num_inference_steps, self.device)\n",
        "      seed = random.randint(1, 1000000000) if seed == -1 else seed\n",
        "\n",
        "      with torch.no_grad():\n",
        "          latents = self.pipe_i2i(\n",
        "              prompt=prompt,\n",
        "              negative_prompt=negative_prompt,\n",
        "              image=image,\n",
        "              num_inference_steps=num_inference_steps,\n",
        "              strength=denoising_strength,\n",
        "              generator=torch.manual_seed(seed),\n",
        "              max_embeddings_multiples=max_embeddings_multiples,\n",
        "              guidance_scale=guidance_scale,\n",
        "              output_type=\"latent\"\n",
        "          ).images # 1x4x(W/8)x(H/8)\n",
        "\n",
        "          if save_path is not None:\n",
        "              pil_image = self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "              os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "              pil_image.save(save_path, quality=95)\n",
        "\n",
        "          if output_type == \"latent\":\n",
        "              return latents\n",
        "          elif output_type == \"pil\":\n",
        "              return self.decode_latents_to_PIL_image(latents, decode_factor)\n",
        "          else:\n",
        "              raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def diffusion_enhance(\n",
        "          self,\n",
        "          prompt,\n",
        "          source_image,\n",
        "          negative_prompt,\n",
        "          scheduler_name=\"dpm++_2m_karras\",\n",
        "          num_inference_steps=20,\n",
        "          num_inference_steps_enhance=20,\n",
        "          guidance_scale=10,\n",
        "          width=512,\n",
        "          height=512,\n",
        "          seed=1234,\n",
        "          max_embeddings_multiples=None,\n",
        "          upscale_target=\"latent\", # \"latent\" or \"pil\"\n",
        "          interpolate_mode=\"nearest\",\n",
        "          antialias = True,\n",
        "          upscale_by=1.8,\n",
        "          enhance_steps=2, # 2=Hires.fix\n",
        "          denoising_strength=0.58,\n",
        "          output_type=\"pil\",\n",
        "          decode_factor=0.15,\n",
        "          decode_factor_final=0.18215,\n",
        "          save_dir=\"output\"\n",
        "          ):\n",
        "\n",
        "      with torch.no_grad():\n",
        "          w_init = calc_pix_8(width)\n",
        "          h_init = calc_pix_8(height)\n",
        "          w_final = calc_pix_8(w_init * upscale_by)\n",
        "          h_final = calc_pix_8(h_init * upscale_by)\n",
        "          resolution_pairs = [(calc_pix_8(x), calc_pix_8(y)) for x, y\n",
        "                  in zip(np.linspace(w_init, w_final, enhance_steps),\n",
        "                          np.linspace(h_init, h_final, enhance_steps))\n",
        "                  ]\n",
        "          image = None\n",
        "          now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "          if enhance_steps == 1: # Single generation\n",
        "              image = self.diffusion_from_noise(\n",
        "                      prompt,\n",
        "                      source_image,\n",
        "                      negative_prompt,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      width=w_final,\n",
        "                      height=h_final,\n",
        "                      output_type=output_type,\n",
        "                      decode_factor=decode_factor_final,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}.jpg\")\n",
        "                  )\n",
        "              return image\n",
        "\n",
        "\n",
        "          for i, (w, h) in enumerate(resolution_pairs):\n",
        "\n",
        "              if image is None: # Step 1: Generate low-quality image\n",
        "                  image = self.diffusion_from_noise(\n",
        "                      prompt,\n",
        "                      source_image,\n",
        "                      negative_prompt,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=num_inference_steps,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      width=w,\n",
        "                      height=h,\n",
        "                      output_type=upscale_target,\n",
        "                      decode_factor=decode_factor,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}_{i}.jpg\")\n",
        "                  )\n",
        "                  continue\n",
        "\n",
        "              # Step 2: Interpolate latent or image -> PIL image\n",
        "              if upscale_target == \"latent\":\n",
        "                  image = torch.nn.functional.interpolate(\n",
        "                          image,\n",
        "                          (h // 8, w // 8),\n",
        "                          mode=interpolate_mode,\n",
        "                          antialias=True if antialias and interpolate_mode != \"nearest\" else False,\n",
        "                      )\n",
        "                  image = self.decode_latents_to_PIL_image(image, decode_factor)\n",
        "              else:\n",
        "                  image = image.resize((w, h), Image.Resampling.LANCZOS)\n",
        "\n",
        "              # Step 3: Generate image (i2i)\n",
        "              if i < len(resolution_pairs) - 1:\n",
        "                  image = self.diffusion_from_image(\n",
        "                      prompt,\n",
        "                      negative_prompt,\n",
        "                      image,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=int(num_inference_steps_enhance / denoising_strength) + 1,\n",
        "                      denoising_strength=denoising_strength,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      output_type=upscale_target,\n",
        "                      decode_factor=decode_factor,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}_{i}.jpg\")\n",
        "                  )\n",
        "\n",
        "              else: # Final enhance\n",
        "                  image = self.diffusion_from_image(\n",
        "                      prompt,\n",
        "                      negative_prompt,\n",
        "                      image,\n",
        "                      scheduler_name=scheduler_name,\n",
        "                      num_inference_steps=int(num_inference_steps_enhance / denoising_strength) + 1,\n",
        "                      denoising_strength=denoising_strength,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      output_type=output_type,\n",
        "                      decode_factor=decode_factor_final,\n",
        "                      seed=seed,\n",
        "                      max_embeddings_multiples=max_embeddings_multiples,\n",
        "                      save_path=os.path.join(save_dir, f\"{now_str}_{i}.jpg\")\n",
        "                  )\n",
        "                  return image\n",
        "\n",
        "  %cd /content/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @markdown ***\n",
        "# @markdown  i2i, controlnet ã«ä½¿ç”¨ã™ã‚‹ç”»åƒã‚’è¨­å®š\n",
        "\n",
        "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹é–¢æ•°\n",
        "def get_jpeg_files(source_dir, extension):\n",
        "    files = [] # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–\n",
        "    for ext in extension: # æ‹¡å¼µå­ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
        "        images_path = os.path.join(source_dir, ext)\n",
        "        files.extend(glob.glob(images_path)) # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
        "    return files\n",
        "\n",
        "\n",
        "# ç”»åƒã¾ãŸã¯ãƒãƒ¼ã‚ºã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹é–¢æ•°\n",
        "def get_images(jpeg_files, mode):\n",
        "    images = []\n",
        "    if mode == \"i2i\":\n",
        "        for file in jpeg_files:\n",
        "            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "            p = Image.open(file)\n",
        "            images.append(p)\n",
        "    elif mode == \"openpose\":\n",
        "        # ãƒãƒ¼ã‚ºã‚’æ¤œå‡ºã™ã‚‹\n",
        "        controlnet_check_path = \"lllyasviel/ControlNet\"\n",
        "        pose_detector = OpenposeDetector.from_pretrained(controlnet_check_path) # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "        for file in jpeg_files:\n",
        "            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚ºã‚’æ¤œå‡ºã™ã‚‹\n",
        "            p = pose_detector(Image.open(file))\n",
        "            # ãƒãƒ¼ã‚ºã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "            images.append(p)\n",
        "    elif mode == \"canny\":\n",
        "        for file in jpeg_files:\n",
        "            # ãƒªã‚¹ãƒˆå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹\n",
        "            image = load_image(file)\n",
        "            image = np.array(image)\n",
        "            # ç”»åƒã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’CV_8Uã«å¤‰æ›ã™ã‚‹\n",
        "            low_threshold, high_threshold = 100, 200\n",
        "            image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "            image = image[:, :, None]\n",
        "            image = np.concatenate([image, image, image], axis=2)\n",
        "            canny_image = Image.fromarray(image)\n",
        "            canny_image\n",
        "            # ãƒãƒ¼ã‚ºã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "            images.append(canny_image)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "    return images\n",
        "\n",
        "\n",
        "# ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹é–¢æ•°\n",
        "def get_controlnet_model_path(mode):\n",
        "    if mode == \"openpose\":\n",
        "        return \"lllyasviel/sd-controlnet-openpose\"\n",
        "    elif mode == \"canny\":\n",
        "        return \"lllyasviel/sd-controlnet-canny\"\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "\n",
        "\n",
        "# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°\n",
        "def load_model(model_path):\n",
        "    # ãƒ‡ãƒ¼ã‚¿å‹ã¯float16ã«æŒ‡å®šã™ã‚‹\n",
        "    return ControlNetModel.from_pretrained(model_path, torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "if isMode.on_i2i or isMode.on_controlnet:\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
        "    source_dir = \"/content/drive/MyDrive/StableDiffusion/ImageSource/wedding\" #@param {type: \"string\"}\n",
        "    extension = \"*.jpg\", \"*.png\", \"*.jpeg\", \"*.webp\" # è¤‡æ•°ã®æ‹¡å¼µå­ã‚’ã‚¿ãƒ—ãƒ«ã§æŒ‡å®š\n",
        "    jpeg_files = get_jpeg_files(source_dir, extension)\n",
        "\n",
        "    # @markdown ***\n",
        "    # @markdown ***Controlnet ã‚’ä½¿ã†å ´åˆã€ã©ã‚Œã‹ã‚’é¸ã¶***\n",
        "    is_openpose = False # @param {type:\"boolean\"}\n",
        "    is_canny = True # @param {type:\"boolean\"}\n",
        "\n",
        "    # for_generate_images = []\n",
        "    # match-caseæ–‡ã§æ¡ä»¶åˆ†å²ã™ã‚‹\n",
        "    match True:\n",
        "        case isMode.on_i2i:\n",
        "            # ç”»åƒã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹\n",
        "            for_generate_images = get_images(jpeg_files, \"i2i\")\n",
        "            # i2i_images = get_images(jpeg_files, \"i2i\")\n",
        "        case isMode.on_controlnet:\n",
        "            # ãƒãƒ¼ã‚ºã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹\n",
        "            for_generate_images = get_images(jpeg_files, \"openpose\" if is_openpose else \"canny\")\n",
        "            # controlnet_images = get_images(jpeg_files, \"openpose\" if is_openpose else \"canny\")\n",
        "            if len(for_generate_images) == 0:\n",
        "                raise ValueError(\"posesãŒç©ºã§ã™\")\n",
        "            # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹\n",
        "            controlnet_model_path = get_controlnet_model_path(\"openpose\" if is_openpose else \"canny\")\n",
        "            # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "            controlnet = load_model(controlnet_model_path)\n",
        "        # case _:\n",
        "        #     rraise ValueError(\"Invalid condition\")\n",
        "\n",
        "\n",
        "# @markdown ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ9NqU4F8UpJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Modelã€LoRA ã‚’ StableDiffusionImageGeneratorMod ã«è¨­å®š { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "#ãƒ¢ãƒ‡ãƒ«\n",
        "# @markdown ***\n",
        "# @markdown ğŸ‘š Model\n",
        "model_dir = \"/content/drive/MyDrive/StableDiffusion/Model/\"\n",
        "model_name = \"beautifulRealistic_v60\" #@param [\"kanpiromix_v20\",\"chilled_remix_v2\", \"emilianJR/XXMix_9realistic\", \"beautifulRealistic_v60\", \"sinkinai/Beautiful-Realistic-Asians-v5\", \"emilianJR/majicMIX_realistic_v6\", \"runwayml/stable-diffusion-v1-5\", \"stablediffusionapi/anything-v5\"] {allow-input: true}\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "# ã€Œ/ã€ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ã€ã€‡ã€‡ã‚’å®Ÿè¡Œã™ã‚‹\n",
        "if \"/\" in model_name:\n",
        "  model_path = model_name\n",
        "\n",
        "# VAE ã‚’èª­ã¿è¾¼ã‚€\n",
        "vae_path = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\",\"stabilityai/sd-vae-ft-ema\", \"rossiyareich/anything-v4.0-vae\"] {allow-input: true}\n",
        "vae=AutoencoderKL.from_pretrained(pretrained_model_name_or_path=vae_path, torch_dtype=torch.float16)\n",
        "\n",
        "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰\n",
        "generator = StableDiffusionImageGeneratorMod(\n",
        "  model_path,\n",
        "  controlnet=controlnet if isMode.on_controlnet else None, # æ¡ä»¶å¼ã‚’ä½¿ã£ã¦å¼•æ•°ã®å€¤ã‚’æ±ºã‚ã‚‹\n",
        "  vae=vae,\n",
        ")\n",
        "\n",
        "# @markdown ***\n",
        "# @markdown ğŸ™ LoRA  â†’  ãƒ•ã‚¡ã‚¤ãƒ«åã«  .safetensors  ã‚’ã¤ã‘ã‚‹ã“ã¨ã€‚\n",
        "lora_base_dir = \"/content/drive/MyDrive/StableDiffusion/Lora/\"\n",
        "\n",
        "isUse_lora_1 = True #@param {type:\"boolean\"}\n",
        "if isUse_lora_1:\n",
        "  lora_1=\"flat2.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_1 = -1 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_1 = os.path.join(lora_base_dir, lora_1)\n",
        "  generator.load_lora(lora_path_1, alpha=lora_alpha_1)\n",
        "\n",
        "isUse_lora_2= True #@param {type:\"boolean\"}\n",
        "if isUse_lora_2:\n",
        "  lora_2=\"haruchanAI_realitic.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_2 = 0.4 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_2 = os.path.join(lora_base_dir, lora_2)\n",
        "  generator.load_lora(lora_path_2, alpha=lora_alpha_2)\n",
        "\n",
        "isUse_lora_3= False #@param {type:\"boolean\"}\n",
        "if isUse_lora_3:\n",
        "  lora_3=\"WeddingDressEXv0.4.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_3 = 0.4 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_3 = os.path.join(lora_base_dir, lora_3)\n",
        "  generator.load_lora(lora_path_3, alpha=lora_alpha_3)\n",
        "\n",
        "isUse_lora_4= True #@param {type:\"boolean\"}\n",
        "if isUse_lora_4:\n",
        "  lora_4=\"koreanDollLikeness_v20.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_4 = 0.5 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_4 = os.path.join(lora_base_dir, lora_4)\n",
        "  generator.load_lora(lora_path_4, alpha=lora_alpha_4)\n",
        "\n",
        "isUse_lora_5= True #@param {type:\"boolean\"}\n",
        "if isUse_lora_5:\n",
        "  lora_5=\"japaneseDollLikeness_v15.safetensors\" #@param [\"flat2\", \"add_detail\", \"koreanDollLikeness_v20\", \"japaneseDollLikeness_v15\", \"handsome_male-02\", \"trueBuruma_v26red\", \"Bukkake\", \"fixStandingDoggyStyle-000017\", \"concept-staged-deepthroat-v3\", \"rum_and_koya\", \"koya_v6\"] {allow-input: true}\n",
        "  lora_alpha_5 = 0.5 # @param {type:\"slider\", min:-1, max:2, step:0.1}\n",
        "  lora_path_5 = os.path.join(lora_base_dir, lora_5)\n",
        "  generator.load_lora(lora_path_5, alpha=lora_alpha_5)\n",
        "\n",
        "# @markdown ***\n",
        "#@markdown ğŸ‘¾ Embeddings\n",
        "embeddings_base_dir = \"/content/drive/MyDrive/StableDiffusion/embeddings/\"\n",
        "\n",
        "isUse_embedding_1 = True #@param {type:\"boolean\"}\n",
        "if isUse_embedding_1:\n",
        "  embedding_1 = \"EasyNegative.safetensors\" #@param {type:\"string\"}\n",
        "  trigger_word_1 = \"EasyNegative\" #@param {type:\"string\"}\n",
        "  safetensors_path_1 = os.path.join(embeddings_base_dir, embedding_1)\n",
        "  generator.load_embeddings(safetensors_path_1, embedding_1, trigger_word_1)\n",
        "\n",
        "isUse_embedding_2 = True #@param {type:\"boolean\"}\n",
        "if isUse_embedding_2:\n",
        "  embedding_2 = \"negative_hand-neg.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_2 = \"negative_hand-neg\" #@param {type:\"string\"}\n",
        "  safetensors_path_2 = os.path.join(embeddings_base_dir, embedding_2)\n",
        "  generator.load_embeddings(safetensors_path_2, embedding_2, trigger_word_2)\n",
        "\n",
        "isUse_embedding_3 = False #@param {type:\"boolean\"}\n",
        "if isUse_embedding_3:\n",
        "  embedding_3 = \"ulzzang-6500-v1.1.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_3 = \"ulzzang-6500-v1.1\" #@param {type:\"string\"}\n",
        "  safetensors_path_3 = os.path.join(embeddings_base_dir, embedding_3)\n",
        "  generator.load_embeddings(safetensors_path_3, embedding_3, trigger_word_3)\n",
        "\n",
        "\n",
        "isUse_embedding_4 = True #@param {type:\"boolean\"}\n",
        "if isUse_embedding_4:\n",
        "  embedding_4 = \"ng_deepnegative_v1_75t.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_4 = \"ng_deepnegative_v1_75t\" #@param {type:\"string\"}\n",
        "  safetensors_path_4 = os.path.join(embeddings_base_dir, embedding_4)\n",
        "  generator.load_embeddings(safetensors_path_4, embedding_4, trigger_word_4)\n",
        "\n",
        "\n",
        "isUse_embedding_5 = False #@param {type:\"boolean\"}\n",
        "if isUse_embedding_5:\n",
        "  embedding_5 = \"GS-DeFeminize-neg.pt\" #@param {type:\"string\"}\n",
        "  trigger_word_5 = \"GS-Masculine, GS-DeFeminize-Neg\" #@param {type:\"string\"}\n",
        "  safetensors_path_5 = os.path.join(embeddings_base_dir, embedding_5)\n",
        "  generator.load_embeddings(safetensors_path_5, embedding_4, trigger_word_5)\n",
        "\n",
        "\n",
        "# @markdown ***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYM1Dv3QeR08",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title ç”»åƒã‚’ç”Ÿæˆ  { display-mode: \"form\" }\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ã†æ—¥ä»˜ã¨æ™‚åˆ»ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å®šç¾©ã™ã‚‹\n",
        "file_format = \"%Y%m%d_%H%M%S\"\n",
        "i=0\n",
        "\n",
        "# ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—\n",
        "jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "#txt2imgå‡ºåŠ›ç”»åƒã®ä¿å­˜å…ˆ\n",
        "#@markdown ***å‡ºåŠ›ç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€***\n",
        "save_dir = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/\"\n",
        "save_folder = \"test-wedding\" #@param {type: \"string\"}\n",
        "save_path = os.path.join(save_dir, save_folder)\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "#@@markdown ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "prompt = \"((masterpiece)), ((super detailed)), photo-realistic, ((best quality)), 1girl, embarrassed face, small Breast, ((standing, full body shot)), (wedding dress, veil:1.4), in beach, SunLight\" #@param {type:\"string\"}\n",
        "#@@markdown ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "negative = \"((negative_hand)), ((negative_hand-neg)), ((ng_deepnegative_v1_75t)), EasyNegative, Simple background:1.5, ((worst picture quality, low quality, normal quality)),  painting, sketch, text, error, jpeg image, logo, signature, watermark, Username, (extra fingers, deformed hands, polydactyl:1.5), long neck, bad proportions\" #@param {type:\"string\"}\n",
        "# ä½¿ç”¨ã™ã‚‹ç”»åƒ\n",
        "if isMode.on_controlnet or isMode.on_i2i:\n",
        "    #@markdown ***ğŸï¸ ç”»åƒ â†’ ãƒ©ãƒ³ãƒ€ãƒ ã«ä½¿ã†ã‹ï¼Ÿ â—¯ç•ªç›®ã‚’ã¤ã‹ã†ã‹ï¼Ÿ***\n",
        "    is_random_image = True #@param {type: \"boolean\"}\n",
        "    image_index = 0 # @param {type:\"slider\", min:0, max:20, step:1}\n",
        "    if image_index <= 0 or image_index > len(for_generate_images):\n",
        "      image_index = 0\n",
        "    else:\n",
        "      image_index -= 1\n",
        "\n",
        "\n",
        "#@markdown ***å‡ºåŠ›æšæ•°***\n",
        "batch_count = 2 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "#@markdown ***ã‚¹ãƒ†ãƒƒãƒ—æ•°***\n",
        "steps = 5 # @param {type:\"slider\", min:5, max:100, step:5}\n",
        "#@@markdown ç”»åƒã‚µã‚¤ã‚º\n",
        "img_width =   512  # @param {type:\"slider\", min:256, max:1920, step:64}\n",
        "img_height =  768  # @param {type:\"slider\", min:256, max:1920, step:64}\n",
        "#@@markdown CFG\n",
        "CFG = 7  # @param {type:\"slider\", min:0, max:25, step:1}\n",
        "#@markdown ***ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©(ã‚µãƒ³ãƒ—ãƒ©ãƒ¼)***\n",
        "scheduler=\"dpm++_2m_karras\" #@param [\"unipc\",\"euler_a\",\"euler\",\"ddim\",\"ddpm\",\"deis\",\"dpm2\",\"dpm++_2s\",\"dpm++_2m\",\"dpm++_2m_karras\",\"dpm++_sde\",\"dpm++_sde_karras\",\"heun\",\"heun_karras\",\"lms\",\"lms_karras\",\"pndm\",\"dpm++_2m_karras\"]\n",
        "#@@markdown ã‚·ãƒ¼ãƒ‰ï¼ˆ-1ã®æ™‚ã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "if seed is None or seed == -1:\n",
        "  inputSeed = random.randint(0, 2147483647)\n",
        "else:\n",
        "  valueSeed = seed\n",
        "#@markdown ***ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’ã©ã®ãã‚‰ã„ç·©å’Œã™ã‚‹ã‹ â†’ 75 Ã— N ã«ãªã‚‹***\n",
        "max_embeddings_multiples=6  # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "#@markdown ***Hires.fix ã®æœ‰åŠ¹åŒ–***\n",
        "hires_fix = True #@param {type: \"boolean\"}\n",
        "enhance_steps = 2 if hires_fix else 1\n",
        "#@markdown ***è§£åƒåº¦å€ç‡(ä¹—ç®—å¾Œã€æœ€ã‚‚è¿‘ã„8ã®å€æ•°ã®ã‚µã‚¤ã‚ºã¨ãªã‚‹)***\n",
        "upscaling_ratio = 2 # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "if hires_fix == False:\n",
        "  upscaling_ratio=1\n",
        "#@markdown ***ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•°***\n",
        "is_test_hires_fix = False #@param {type: \"boolean\"}\n",
        "up_steps = 5 # @param {type:\"slider\", min:5, max:100, step:5}\n",
        "if is_test_hires_fix == True:\n",
        "  up_steps = 10\n",
        "#@markdown ***å°ã•ã„ã»ã©å…ƒç”»åƒã‚’å°Šé‡ï¼‰ ï¼ˆ0.5ï½0.7ï¼‰ãŒæ¨å¥¨***\n",
        "denoising_strength = 0.6 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "\n",
        "\n",
        "# ç”»åƒç”Ÿæˆ\n",
        "for i in range(batch_count):\n",
        "  if seed is None or seed == -1:valueSeed = inputSeed + i\n",
        "  if isMode.on_controlnet or isMode.on_i2i:\n",
        "      if is_random_image and len(for_generate_images) > 0:\n",
        "          image_index = random.randint(0, len(for_generate_images)-1)\n",
        "          source_image = for_generate_images[image_index]\n",
        "\n",
        "  image = generator.diffusion_enhance(\n",
        "    prompt,\n",
        "    source_image=source_image if source_image else None,\n",
        "    negative_prompt=negative,\n",
        "    scheduler_name=scheduler,\n",
        "    num_inference_steps=steps,\n",
        "    num_inference_steps_enhance=up_steps,\n",
        "    guidance_scale=CFG,\n",
        "    width=img_width,\n",
        "    height=img_height,\n",
        "    seed=valueSeed,\n",
        "    max_embeddings_multiples=max_embeddings_multiples,\n",
        "    upscale_target=\"latent\",\n",
        "    interpolate_mode=\"bicubic\",\n",
        "    antialias=True,\n",
        "    upscale_by=upscaling_ratio,\n",
        "    enhance_steps=enhance_steps,\n",
        "    denoising_strength=denoising_strength,\n",
        "    output_type=\"pil\",\n",
        "    decode_factor=0.15,\n",
        "    decode_factor_final=0.18215,\n",
        "  )\n",
        "\n",
        "  # ç”»åƒã‚’ä¿å­˜\n",
        "\n",
        "  # ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—\n",
        "  jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  #å‡ºåŠ›ã™ã‚‹ç”»åƒã®åå‰ã‚’ç”Ÿæˆã™ã‚‹\n",
        "  file_name = (jst_now.strftime(file_format)+ \"_\" + str(valueSeed))\n",
        "  image_name = file_name + f\".png\"\n",
        "\n",
        "  #ç”»åƒã‚’ä¿å­˜ã™ã‚‹\n",
        "  save_location = os.path.join(save_path, image_name)\n",
        "\n",
        "  #@markdown ***ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿***\n",
        "  save_metadata = True #@param {type: \"boolean\"}\n",
        "  if save_metadata:\n",
        "    metadata = PngInfo()\n",
        "    # Model\n",
        "    metadata.add_text(\"model\",(model_name))\n",
        "    # Lora\n",
        "    if isUse_lora_1:\n",
        "      metadata.add_text(\"lora_1\",(lora_1))\n",
        "      metadata.add_text(\"lora_alpha_1\",(str(lora_alpha_1)))\n",
        "    if isUse_lora_2:\n",
        "      metadata.add_text(\"lora_2\",(lora_2))\n",
        "      metadata.add_text(\"lora_alpha_2\",(str(lora_alpha_2)))\n",
        "    if isUse_lora_3:\n",
        "      metadata.add_text(\"lora_3\",(lora_3))\n",
        "      metadata.add_text(\"lora_alpha_3\",(str(lora_alpha_3)))\n",
        "    if isUse_lora_4:\n",
        "      metadata.add_text(\"lora_4\",(lora_4))\n",
        "      metadata.add_text(\"lora_alpha_4\",(str(lora_alpha_4)))\n",
        "    if isUse_lora_5:\n",
        "      metadata.add_text(\"lora_5\",(lora_5))\n",
        "      metadata.add_text(\"lora_alpha_5\",(str(lora_alpha_5)))\n",
        "    # embedding\n",
        "    if isUse_embedding_1:\n",
        "      metadata.add_text(\"embedding_1\",(embedding_1))\n",
        "      metadata.add_text(\"trigger_word_1\",(str(trigger_word_1)))\n",
        "    if isUse_embedding_2:\n",
        "      metadata.add_text(\"embedding_2\",(embedding_2))\n",
        "      metadata.add_text(\"trigger_word_2\",(str(trigger_word_2)))\n",
        "    if isUse_embedding_3:\n",
        "      metadata.add_text(\"embedding_3\",(embedding_3))\n",
        "      metadata.add_text(\"trigger_word_3\",(str(trigger_word_3)))\n",
        "    if isUse_embedding_4:\n",
        "      metadata.add_text(\"embedding_4\",(embedding_4))\n",
        "      metadata.add_text(\"trigger_word_4\",(str(trigger_word_4)))\n",
        "    if isUse_embedding_5:\n",
        "      metadata.add_text(\"embedding_5\",(embedding_5))\n",
        "      metadata.add_text(\"trigger_word_5\",(str(trigger_word_5)))\n",
        "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "    metadata.add_text(\"prompt\",(prompt))\n",
        "    metadata.add_text(\"negative\",(negative))\n",
        "    metadata.add_text(\"scheduler\",(scheduler))\n",
        "    metadata.add_text(\"steps\",(str(steps)))\n",
        "    metadata.add_text(\"CFG\",(str(CFG)))\n",
        "    metadata.add_text(\"width\",(str(img_width)))\n",
        "    metadata.add_text(\"height\",(str(img_height)))\n",
        "    metadata.add_text(\"seed\",str((valueSeed)))\n",
        "  if hires_fix:\n",
        "    metadata.add_text(\"upscaling ratio\",str((upscaling_ratio)))\n",
        "    metadata.add_text(\"up steps\",str((up_steps)))\n",
        "    metadata.add_text(\"denoising strength\",str((denoising_strength)))\n",
        "    image.save(save_location, pnginfo=metadata)\n",
        "  else:\n",
        "    image.save(save_location)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kbLsCgi4Uyzo"
      },
      "outputs": [],
      "source": [
        "# @title ç”»åƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›\n",
        "# Driveã«æ¥ç¶š\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "isConectDrive=False  #@param {type: \"boolean\"}\n",
        "if isConectDrive:\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#@markdown **ä¿å­˜ã—ãŸç”»åƒã®ãƒ‘ã‚¹**\n",
        "file_dir = \"/content/drive/MyDrive/StableDiffusion/txt2img_output\"  #@param {type: \"string\"}\n",
        "file_name = \"20230726_195024_606196166.png\" #@param {type: \"string\"}\n",
        "file_path = os.path.join(file_dir, file_name)\n",
        "if file_path:\n",
        "  img = Image.open(file_path)\n",
        "\n",
        "  # Model\n",
        "  if \"model\" in img.text:\n",
        "    print(\"model: \",img.text[\"model\"])\n",
        "  # LoRA\n",
        "  if \"lora_1\" in img.text:\n",
        "    print(\"lora_1: \",img.text[\"lora_1\"])\n",
        "    print(\"lora_alpha_1: \", img.text[\"lora_alpha_1\"])\n",
        "  if \"lora_2\" in img.text:\n",
        "    print(\"lora_2: \",img.text[\"lora_2\"])\n",
        "    print(\"lora_alpha_2: \", img.text[\"lora_alpha_2\"])\n",
        "  if \"lora_3\" in img.text:\n",
        "    print(\"lora_3: \",img.text[\"lora_3\"])\n",
        "    print(\"lora_alpha_3: \", img.text[\"lora_alpha_3\"])\n",
        "  if \"lora_4\" in img.text:\n",
        "    print(\"lora_4: \",img.text[\"lora_4\"])\n",
        "    print(\"lora_alpha_4: \", img.text[\"lora_alpha_4\"])\n",
        "  if \"lora_5\" in img.text:\n",
        "    print(\"lora_5: \",img.text[\"lora_5\"])\n",
        "    print(\"lora_alpha_5: \", img.text[\"lora_alpha_5\"])\n",
        "  # embedding\n",
        "  if \"embedding_1\" in img.text:\n",
        "    print(\"embedding_1: \",img.text[\"embedding_1\"])\n",
        "    print(\"trigger_word_1: \", img.text[\"trigger_word_1\"])\n",
        "  if \"embedding_2\" in img.text:\n",
        "    print(\"embedding_2: \",img.text[\"embedding_2\"])\n",
        "    print(\"trigger_word_2: \", img.text[\"trigger_word_2\"])\n",
        "  if \"embedding_3\" in img.text:\n",
        "    print(\"embedding_3: \",img.text[\"embedding_3\"])\n",
        "    print(\"trigger_word_3: \", img.text[\"trigger_word_3\"])\n",
        "  if \"embedding_4\" in img.text:\n",
        "    print(\"embedding_4: \",img.text[\"embedding_4\"])\n",
        "    print(\"trigger_word_4: \", img.text[\"trigger_word_4\"])\n",
        "  if \"embedding_5\" in img.text:\n",
        "    print(\"embedding_5: \",img.text[\"embedding_5\"])\n",
        "    print(\"trigger_word_5: \", img.text[\"trigger_word_5\"])\n",
        "  # Prompt\n",
        "  print(\"Prompt: \",img.text[\"prompt\"])\n",
        "  print(\"Negative Prompt: \",img.text[\"negative\"])\n",
        "  if \"scheduler\" in img.text:\n",
        "    print(\"Scheduler: \", img.text[\"scheduler\"])\n",
        "  print(\"Steps: \",img.text[\"steps\"])\n",
        "  print(\"CFG: \",img.text[\"CFG\"])\n",
        "  print(\"Width: \",img.text[\"width\"])\n",
        "  print(\"Height: \",img.text[\"height\"])\n",
        "  print(\"Seed: \",img.text[\"seed\"])\n",
        "  # Hires.fix\n",
        "  try:\n",
        "    print(\"Upscaling ratio; \",img.text[\"upscaling ratio\"])\n",
        "    print(\"Up steps: \",img.text[\"up steps\"])\n",
        "    print(\"Denoising strength: \",img.text[\"denoising strength\"])\n",
        "  except:\n",
        "    print(\"Hires.fix was OFF.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ila9x3rQUyzo"
      },
      "outputs": [],
      "source": [
        "# ç”»åƒå‡¦ç†ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# globãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import glob\n",
        "\n",
        "# natsortã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
        "from natsort import natsorted\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "isConectDrive=False  #@param {type: \"boolean\"}\n",
        "if isConectDrive:\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ãƒªã‚¹ãƒˆã«æ ¼ç´\n",
        "# ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ã€ãƒ•ã‚©ãƒ«ãƒ€åãŒimagesã§ã€PNGç”»åƒã‚’ã™ã¹ã¦æŒ‡å®šã™ã‚‹\n",
        "path = \"/content/drive/MyDrive/StableDiffusion/txt2img_output/grid/*\" #@param {type:\"string\"}\n",
        "image_paths = []\n",
        "for ext in [\"png\", \"jpeg\", \"jpg\"]:\n",
        "  image_paths.extend(glob.glob(path + \".\" + ext))\n",
        "\n",
        "\n",
        "if image_paths:\n",
        "    # ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã§è‡ªç„¶é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹\n",
        "    image_paths = natsorted(image_paths)\n",
        "\n",
        "    # ç”»åƒã®é«˜ã•ã®æœ€å°å€¤ã¨å¹…ã®æœ€å¤§å€¤ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
        "    min_height = float(\"inf\")\n",
        "    # min_height = 0 # æœ€å°å€¤ã‚’0ã«è¨­å®š\n",
        "    max_width = 0\n",
        "\n",
        "    # image_pathsã®ç”»åƒã®ä¸­ã§ä¸€ç•ªé«˜ã•ã®ä½ã„ç”»åƒã®é«˜ã•ã¨ä¸€ç•ªå¹…ã®å¤§ãã„ç”»åƒã®å¹…ã‚’å–å¾—ã™ã‚‹\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path) # ç”»åƒã‚’èª­ã¿è¾¼ã‚€\n",
        "        h, w, c = img.shape # ç”»åƒã®é«˜ã•ã€å¹…ã€ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’å–å¾—ã™ã‚‹\n",
        "        if h < min_height: # ã‚‚ã—é«˜ã•ãŒæœ€å°å€¤ã‚ˆã‚Šå°ã•ã„å ´åˆã¯\n",
        "            min_height = h # æœ€å°å€¤ã‚’æ›´æ–°ã™ã‚‹\n",
        "        if w > max_width: # ã‚‚ã—å¹…ãŒæœ€å¤§å€¤ã‚ˆã‚Šå¤§ãã„å ´åˆã¯\n",
        "            max_width = w # æœ€å¤§å€¤ã‚’æ›´æ–°ã™ã‚‹\n",
        "\n",
        "\n",
        "    isAddBlack = False #@param {type: \"boolean\"}\n",
        "\n",
        "    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€ãƒªã‚µã‚¤ã‚ºã—ã¦ãƒªã‚¹ãƒˆã«æ ¼ç´\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = cv2.imread(path) # ç”»åƒã‚’èª­ã¿è¾¼ã‚€\n",
        "        h, w, c = img.shape # ç”»åƒã®é«˜ã•ã€å¹…ã€ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’å–å¾—ã™ã‚‹\n",
        "        scale = min_height / h # ç¸®å°ç‡ã‚’è¨ˆç®—ã™ã‚‹\n",
        "        new_h = min_height # æ–°ã—ã„é«˜ã•ã¯æœ€å°å€¤ã¨ã™ã‚‹\n",
        "        new_w = int(w * scale) # æ–°ã—ã„å¹…ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "        img = cv2.resize(img, (new_w, new_h)) # ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã™ã‚‹\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGRã‹ã‚‰RGBã«è‰²ç©ºé–“ã‚’å¤‰æ›ã™ã‚‹\n",
        "\n",
        "        # # ã‚‚ã—å¹…ãŒæœ€å¤§å€¤ã‚ˆã‚Šå°ã•ã„å ´åˆã¯ã€å³å´ã«é»’ã„ä½™ç™½ã‚’è¿½åŠ ã™ã‚‹\n",
        "        # if new_w < max_width:\n",
        "        #     pad_width = max_width - new_w # ä½™ç™½ã®å¹…ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "        #     pad_img = np.zeros((new_h, pad_width, 3), dtype=np.uint8) # é»’ã„ä½™ç™½ã®ç”»åƒã‚’ä½œã‚‹\n",
        "        #     img = np.hstack((img, pad_img)) # å…ƒã®ç”»åƒã¨ä½™ç™½ã®ç”»åƒã‚’æ°´å¹³æ–¹å‘ã«çµåˆã™ã‚‹\n",
        "\n",
        "        if isAddBlack:\n",
        "          # ã‚‚ã—å¹…ãŒæœ€å¤§å€¤ã‚ˆã‚Šå°ã•ã„å ´åˆã¯ã€å·¦å³ã«é»’ã„ä½™ç™½ã‚’è¿½åŠ ã™ã‚‹\n",
        "          if new_w < max_width:\n",
        "              pad_width = max_width - new_w # ä½™ç™½ã®å¹…ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "              pad_left = pad_width // 2 # å·¦å´ã®ä½™ç™½ã®å¹…ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "              pad_right = pad_width - pad_left # å³å´ã®ä½™ç™½ã®å¹…ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "              pad_img_left = np.zeros((new_h, pad_left, 3), dtype=np.uint8) # å·¦å´ã®é»’ã„ä½™ç™½ã®ç”»åƒã‚’ä½œã‚‹\n",
        "              pad_img_right = np.zeros((new_h, pad_right, 3), dtype=np.uint8) # å³å´ã®é»’ã„ä½™ç™½ã®ç”»åƒã‚’ä½œã‚‹\n",
        "              img = np.hstack((pad_img_left, img, pad_img_right)) # å·¦å´ã®ä½™ç™½ã€å…ƒã®ç”»åƒã€å³å´ã®ä½™ç™½ã‚’æ°´å¹³æ–¹å‘ã«çµåˆã™ã‚‹\n",
        "\n",
        "        images.append(img) # ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "\n",
        "\n",
        "    # ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š\n",
        "    # åˆ—æ•°\n",
        "    rows = 2 #@param {type:\"integer\"}\n",
        "    # è¡Œæ•°\n",
        "    cols = 2 #@param {type:\"integer\"}\n",
        "    required_images = 4 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "    # ç”»åƒãŒè¶³ã‚Šãªã„å ´åˆã¯ã€é»’ã„ç”»åƒã‚’ä½œã£ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "    num_images = len(images) # ãƒªã‚¹ãƒˆã«æ ¼ç´ã•ã‚ŒãŸç”»åƒã®æ•°\n",
        "    if num_images < required_images: # ã‚‚ã—è¶³ã‚Šãªã„å ´åˆã¯\n",
        "        for i in range(required_images - num_images): # è¶³ã‚Šãªã„åˆ†ã ã‘ç¹°ã‚Šè¿”ã™ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "            black_img = np.zeros((int(min_height), int(max_width), 3), dtype=np.uint8) # é»’ã„ç”»åƒã‚’ä½œã‚‹ï¼ˆæ•´æ•°å‹ã«å¤‰æ›ï¼‰\n",
        "            images.append(black_img) # ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "\n",
        "    # ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’å®šç¾©\n",
        "    def grid_display(images, rows, cols):\n",
        "        # ç”»åƒã®æ•°ã¨è¡Œåˆ—æ•°ãŒä¸€è‡´ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™\n",
        "        assert len(images) == rows*cols\n",
        "\n",
        "        # è¡Œåˆ—æ•°ã«å¿œã˜ã¦ç”»åƒã‚’çµåˆã™ã‚‹\n",
        "        # ã¾ãšã¯è¡Œã”ã¨ã«ç”»åƒã‚’æ°´å¹³æ–¹å‘ã«çµåˆã™ã‚‹\n",
        "        horizontal = []\n",
        "        for i in range(rows):\n",
        "            start_index = i*cols # è¡Œã®å…ˆé ­ã®ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
        "            end_index = start_index + cols # è¡Œã®æœ€å¾Œã®ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
        "            horizontal.append(np.hstack(images[start_index:end_index])) # æ°´å¹³æ–¹å‘ã«çµåˆã—ãŸç”»åƒã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹\n",
        "\n",
        "        # æ¬¡ã«ã€çµåˆã—ãŸè¡Œã”ã¨ã®ç”»åƒã‚’å‚ç›´æ–¹å‘ã«çµåˆã™ã‚‹\n",
        "        vertical = np.vstack(horizontal) # å‚ç›´æ–¹å‘ã«çµåˆã—ãŸç”»åƒ\n",
        "\n",
        "        # çµæœã‚’è¿”ã™\n",
        "        return vertical\n",
        "\n",
        "    # é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã€ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºã™ã‚‹\n",
        "    result = grid_display(images, cols, rows) # é–¢æ•°ã‚’å‘¼ã³å‡ºã™\n",
        "    plt.figure(figsize=(50,50)) # å›³ã®ã‚µã‚¤ã‚ºã‚’è¨­å®šã™ã‚‹\n",
        "    plt.imshow(result) # å›³ã«ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹ï¼ˆè£œé–“ãªã—ï¼‰\n",
        "    plt.axis(\"off\") # è»¸ã‚’éè¡¨ç¤ºã«ã™ã‚‹\n",
        "    plt.show() # å›³ã‚’è¡¨ç¤ºã™ã‚‹\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart session\n",
        "#@markdown Executing this cell crashes session. It's intended.<br>You don't need to re-execute cells above.\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hcMkiFDQYzHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}